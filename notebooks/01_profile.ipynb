{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f537850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 1  – Ingestion & Baseline Validation\n",
    "----------------------------------------\n",
    "Loads an Excel/CSV file into a DataFrame, confirms the schema,\n",
    "and applies first‑pass dtype + null handling rules.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd                          # pandas gives us DataFrame + Excel utilities\n",
    "from pathlib import Path                     # convenient cross‑platform path handling\n",
    "\n",
    "\n",
    "def load_contacts(file_path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a contacts file and return a validated, typed DataFrame.\"\"\"\n",
    "    \n",
    "    file_path = Path(file_path)                                                  # ensure we have a Path object\n",
    "    if not file_path.exists():                                                   # guard against bad path\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # --- 1.1  Define the required schema -------------------------------------\n",
    "    required_cols = [                                                            # columns the downstream logic expects\n",
    "        \"Account Name\", \"Full Name\", \"Email\", \"Contact Id\",\n",
    "        \"Admin Role\", \"Primary Contact\", \"Active Contact\",\n",
    "        \"ConnectLink Status\", \"Connect Link Email\",\n",
    "        \"# of cases\", \"# of opps\",\n",
    "        \"Last Activity\", \"Created Date\"\n",
    "    ]\n",
    "    \n",
    "    # (optional) explicit dtype map for critical fields\n",
    "    dtype_map = {                                                                # enforce string types where ID semantics matter\n",
    "        \"Account Name\":  \"string\",\n",
    "        \"Full Name\":     \"string\",\n",
    "        \"Email\":         \"string\",\n",
    "        \"Contact Id\":    \"string\",\n",
    "        \"Admin Role\":    \"string\",\n",
    "        \"Primary Contact\": \"boolean\",                                            # stored as Pandas BooleanDtype (allows NA)\n",
    "        \"Active Contact\": \"string\",\n",
    "        \"ConnectLink Status\": \"string\",\n",
    "        \"Connect Link Email\": \"string\",\n",
    "        \"# of cases\":   \"Int64\",                                                 # nullable integer\n",
    "        \"# of opps\":    \"Int64\"\n",
    "    }\n",
    "    \n",
    "    date_cols = [\"Last Activity\", \"Created Date\"]                                # columns that should parse as dates\n",
    "    \n",
    "    # --- 1.2  Load the file ---------------------------------------------------\n",
    "    df = pd.read_excel(                                                          # read the Excel file into a DataFrame\n",
    "        file_path,\n",
    "        dtype=dtype_map,                                                         # apply dtypes where safe\n",
    "        parse_dates=date_cols,                                                   # let pandas parse these as datetime\n",
    "        engine=\"openpyxl\"                                                        # use openpyxl backend (already installed)\n",
    "    )\n",
    "    \n",
    "    # --- 1.3  Schema validation ----------------------------------------------\n",
    "    missing = [c for c in required_cols if c not in df.columns]                 # find any required columns that are absent\n",
    "    if missing:                                                                 # if list not empty, we cannot proceed\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    \n",
    "    # Re‑order columns to a canonical order (optional but nice‑to‑have)\n",
    "    df = df[required_cols]                                                      # slices DataFrame to the exact order\n",
    "    \n",
    "    # --- 1.4  Null / blank handling ------------------------------------------\n",
    "    df = df.fillna({c: \"\" for c in df.select_dtypes(\"string\").columns})         # replace NaN in string columns with empty string\n",
    "    df[\"Primary Contact\"] = df[\"Primary Contact\"].fillna(False)                 # treat missing boolean as False\n",
    "    # numeric & date columns keep their NA state for now (handled later)\n",
    "    \n",
    "    # --- 1.5  Trim whitespace + lower‑case key text fields --------------------\n",
    "    text_cols = [\"Account Name\", \"Full Name\", \"Email\", \"Connect Link Email\"]\n",
    "    for col in text_cols:\n",
    "        df[col] = (df[col]                                                      # chain string methods safely\n",
    "                     .str.strip()                                               # remove leading/trailing spaces\n",
    "                     .str.replace(r\"\\s+\", \" \", regex=True)                      # collapse internal multiple spaces\n",
    "                 )\n",
    "    \n",
    "    return df                                                                   # DataFrame is now ready for step 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bc8e2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 3 – Hierarchy Weighting\n",
    "----------------------------\n",
    "Given a DataFrame that already passed STEP 1 (ingestion / validation),\n",
    "calculate a numeric `rank_score` for every row based on the business\n",
    "rules you outlined.  Higher `rank_score` ⇒ record is preferred to keep.\n",
    "\n",
    "Return the same DataFrame with extra helper columns that downstream\n",
    "logic (duplicate clustering / merge) can use.\n",
    "\n",
    "Assumes columns are clean, trimmed, and dtype‑safe.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Helper:  map free‑text strings to ordinal weights             |\n",
    "# -------------------------------------------------------------\n",
    "ADMIN_ROLE_MAP    = {\"owner\": 3, \"admin\": 2}                   # anything else → 0\n",
    "CONNECT_STATUS_MAP = {\"A\": 2, \"I\": 1, \"U\": 0}                  # blank / other → −1\n",
    "OPPS_CAP          = 99                                         # avoid giant scores\n",
    "\n",
    "def add_hierarchy_weights(df: pd.DataFrame,\n",
    "                          today: Optional[pd.Timestamp] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach weight columns + aggregate rank_score to *df* and return it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Cleaned contacts table from STEP 1.\n",
    "    today : pd.Timestamp, optional\n",
    "        Override 'now' for deterministic testing; default = current date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Same DataFrame with new columns:\n",
    "            w_admin, w_primary, w_active, w_connect,\n",
    "            w_opps, w_last_activity, w_created, rank_score\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------------------\n",
    "    # 0)  Establish 'today' for date arithmetic                |\n",
    "    # ---------------------------------------------------------\n",
    "    if today is None:\n",
    "        today = pd.Timestamp.today().normalize()               # midnight today\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1)  Normalise text fields for case/whitespace            |\n",
    "    # ---------------------------------------------------------\n",
    "    # lower‑case Admin Role so 'Owner', 'OWNER', etc. match\n",
    "    df['Admin Role_std']         = df['Admin Role'].str.lower().str.strip()\n",
    "\n",
    "    # lower‑case Active Contact to catch 'active', 'Active', etc.\n",
    "    df['Active Contact_std']     = df['Active Contact'].str.lower().str.strip()\n",
    "\n",
    "    # upper‑case ConnectLink Status so A/I/U are consistent\n",
    "    df['ConnectLink Status_std'] = df['ConnectLink Status'].str.upper().str.strip()\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2)  Compute individual weight columns                    |\n",
    "    # ---------------------------------------------------------\n",
    "    # -- 2.1 Admin / Owner weight -----------------------------\n",
    "    df['w_admin'] = (\n",
    "        df['Admin Role_std']\n",
    "          .map(ADMIN_ROLE_MAP)                                  # owner→3, admin→2\n",
    "          .fillna(0)                                            # everything else→0\n",
    "          .astype(int)\n",
    "    )\n",
    "\n",
    "    # -- 2.2 Primary Contact (Boolean) ------------------------\n",
    "    # pandas BooleanDtype stores True/False/NA; treat NA as False (0)\n",
    "    df['w_primary'] = df['Primary Contact'].fillna(False).astype(int)\n",
    "\n",
    "    # -- 2.3 Active Contact -----------------------------------\n",
    "    df['w_active'] = (df['Active Contact_std'] == 'active').astype(int)\n",
    "\n",
    "    # -- 2.4 ConnectLink Status -------------------------------\n",
    "    df['w_connect'] = (\n",
    "        df['ConnectLink Status_std']\n",
    "          .map(CONNECT_STATUS_MAP)                              # A→2, I→1, U→0\n",
    "          .fillna(-1)                                           # blank / unknown→‑1\n",
    "          .astype(int)\n",
    "    )\n",
    "\n",
    "    # -- 2.5 # of Opportunities -------------------------------\n",
    "    df['w_opps'] = (\n",
    "        df['# of opps']\n",
    "          .fillna(0)                                            # NA → 0\n",
    "          .clip(upper=OPPS_CAP)                                 # cap outliers\n",
    "          .astype(int)\n",
    "    )\n",
    "\n",
    "    # -- 2.6 Last Activity (more recent = better) -------------\n",
    "    df['days_since_last'] = (today - df['Last Activity']).dt.days\n",
    "    df['w_last_activity'] = (\n",
    "        -df['days_since_last']                                  # negate so recent → positive\n",
    "          .fillna(-9999)                                        # missing date gets big negative weight\n",
    "          .astype(int)\n",
    "    )\n",
    "\n",
    "    # -- 2.7 Created Date (older = better) --------------------\n",
    "    df['days_since_created'] = (today - df['Created Date']).dt.days\n",
    "    df['w_created'] = (\n",
    "        df['days_since_created']\n",
    "          .fillna(0)                                            # missing date → 0 (neutral)\n",
    "          .astype(int)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3)  Aggregate into a single rank_score                   |\n",
    "    # ---------------------------------------------------------\n",
    "    weight_cols = [\n",
    "        'w_admin', 'w_primary', 'w_active', 'w_connect',\n",
    "        'w_opps', 'w_last_activity', 'w_created'\n",
    "    ]\n",
    "\n",
    "    df['rank_score'] = df[weight_cols].sum(axis=1)              # simple additive model\n",
    "\n",
    "    # Optional: drop helper std/Δ columns to keep DataFrame tidy\n",
    "    df.drop(columns=[\n",
    "        'Admin Role_std', 'Active Contact_std',\n",
    "        'ConnectLink Status_std', 'days_since_last', 'days_since_created'\n",
    "    ], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "582bbe44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account Name</th>\n",
       "      <th>Full Name</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Westdale School</td>\n",
       "      <td>Randall Knox</td>\n",
       "      <td>4077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Westdale School</td>\n",
       "      <td>Randall Knox</td>\n",
       "      <td>908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Westdale School</td>\n",
       "      <td>Randall Knox</td>\n",
       "      <td>3849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Westdale School</td>\n",
       "      <td>Randall Knox</td>\n",
       "      <td>5718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Westdale School</td>\n",
       "      <td>Diana Reed</td>\n",
       "      <td>6151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Account Name     Full Name  rank_score\n",
       "0  Westdale School  Randall Knox        4077\n",
       "1  Westdale School  Randall Knox         908\n",
       "2  Westdale School  Randall Knox        3849\n",
       "3  Westdale School  Randall Knox        5718\n",
       "4  Westdale School    Diana Reed        6151"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_contacts(\"../data/duplicate_contacts_small.xlsx\")  # STEP 1 from earlier\n",
    "df = add_hierarchy_weights(df)                               # STEP 3 here\n",
    "df.head()[['Account Name', 'Full Name', 'rank_score']]       # quick sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6708e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 3b – Generate hierarchy‑comparison tag\n",
    "-------------------------------------------\n",
    "Creates a compact, lexicographically sortable tag that encodes every gate\n",
    "in the business ladder.  Any record whose Admin Role is *Owner* or *Admin*\n",
    "is flagged as `is_privileged=True` and given a sentinel tag; these rows are\n",
    "removed from rank competitions and always kept.\n",
    "\n",
    "Assumes: DataFrame columns already cleaned (trimmed / lower‑ or upper‑cased\n",
    "where relevant) and date columns parsed as datetime64[ns] (see STEP 1).\n",
    "\n",
    "Usage\n",
    "-----\n",
    "df = load_contacts(...)              # STEP 1\n",
    "df = add_comparison_tag(df)          # this step\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Helper – map ConnectLink → tier (string for easy concat)     |\n",
    "# -------------------------------------------------------------\n",
    "CONNECT_TIER = {\n",
    "    \"A\": \"3\",        # Best\n",
    "    \"I\": \"2\",        # Middle  (ties U initially)\n",
    "    \"U\": \"2\",        # Middle  – may be demoted later\n",
    "    \"\":  \"1\"         # Blank / unknown\n",
    "}\n",
    "\n",
    "def add_comparison_tag(df: pd.DataFrame,\n",
    "                       today: datetime | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return *df* with two new columns:\n",
    "        • is_privileged  (bool)  – Owner/Admin rows\n",
    "        • hier_tag       (str)   – hierarchy comparison key\n",
    "    \"\"\"\n",
    "    df = df.copy()                                               # avoid mutating caller’s frame\n",
    "    \n",
    "    if today is None:                                            # allow deterministic unit tests\n",
    "        today = pd.Timestamp.today().normalize()\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 1) Privileged flag  (Owner OR Admin)                     |\n",
    "    # ---------------------------------------------------------\n",
    "    df[\"Admin Role_std\"] = (\n",
    "        df[\"Admin Role\"].astype(str).str.lower().str.strip()\n",
    "    )\n",
    "    df[\"is_privileged\"] = df[\"Admin Role_std\"].isin({\"owner\", \"admin\"})\n",
    "    \n",
    "    # For privileged rows we’ll later assign tag 'PRIV'; for others we build full tag.\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 2) Primary / Active bits                                |\n",
    "    # ---------------------------------------------------------\n",
    "    df[\"primary_bit\"] = df[\"Primary Contact\"].fillna(False).astype(int)         # 1/0\n",
    "    df[\"active_bit\"]  = (df[\"Active Contact\"]\n",
    "                           .astype(str)\n",
    "                           .str.lower()\n",
    "                           .str.strip()\n",
    "                           .eq(\"active\")\n",
    "                           .astype(int))                                        # 1 if 'active'\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 3) ConnectLink tier                                     |\n",
    "    # ---------------------------------------------------------\n",
    "    df[\"connect_raw\"] = df[\"ConnectLink Status\"].astype(str).str.upper().str.strip()\n",
    "    df[\"connect_tier\"] = df[\"connect_raw\"].map(CONNECT_TIER).fillna(\"1\")        # default blank tier\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 4) Opportunities bucket                                 |\n",
    "    # ---------------------------------------------------------\n",
    "    opps = df[\"# of opps\"].fillna(0).astype(int)\n",
    "    df[\"opps_bucket\"] = pd.cut(\n",
    "        opps,\n",
    "        bins=[-1, 0, 3, float(\"inf\")],            # (-1,0]  (0,3]  (3,∞)\n",
    "        labels=[\"Z\", \"L\", \"H\"]                    # Zero, Low, High(≥4)\n",
    "    ).astype(str)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 5) Last‑activity tier                                   |\n",
    "    # ---------------------------------------------------------\n",
    "    days_since_last = (today - df[\"Last Activity\"]).dt.days\n",
    "    df[\"activity_tier\"] = pd.cut(\n",
    "        days_since_last,\n",
    "        bins=[-float(\"inf\"), 365, 912, float(\"inf\")],    # recent, mid, stale\n",
    "        labels=[\"1\", \"2\", \"3\"]                          # smaller = better\n",
    "    ).astype(str)\n",
    "    df[\"activity_tier\"] = df[\"activity_tier\"].where(~days_since_last.isna(), \"4\")  # '4' for NaT\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 6) Conditional demotion for Connect = 'U'               |\n",
    "    #     • If U has opps_bucket == 'Z' AND activity_tier in  |\n",
    "    #       {'3','4'}, downgrade its connect_tier to '1'.     |\n",
    "    # ---------------------------------------------------------\n",
    "    mask_demote_u = (\n",
    "        (df[\"connect_raw\"] == \"U\") &\n",
    "        (df[\"opps_bucket\"] == \"Z\") &\n",
    "        (df[\"activity_tier\"].isin({\"3\", \"4\"}))\n",
    "    )\n",
    "    df.loc[mask_demote_u, \"connect_tier\"] = \"1\"                     # now ties blank\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 7) Email‑presence bit  (only breaks ties later but       |\n",
    "    #    encode now for consistency)                           |\n",
    "    # ---------------------------------------------------------\n",
    "    df[\"email_bit\"] = (\n",
    "        df[\"Email\"]\n",
    "          .astype(str)\n",
    "          .str.strip()\n",
    "          .ne(\"\")                # non‑empty string?\n",
    "          .astype(int)\n",
    "    )\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 8) Created‑date rank  (older = bigger number)            |\n",
    "    # ---------------------------------------------------------\n",
    "    days_since_created = (today - df[\"Created Date\"]).dt.days.astype(\"Int64\")\n",
    "    # cap to 5 digits; pad to fixed width so lexicographic sort works\n",
    "    df[\"created_rank\"] = (\n",
    "        days_since_created\n",
    "          .fillna(0)\n",
    "          .clip(lower=0, upper=99999)\n",
    "          .astype(int)\n",
    "          .astype(str)\n",
    "          .str.zfill(5)          # e.g., 00234  > 00007\n",
    "    )\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 9) Compose hierarchy tag                                |\n",
    "    #    Format: role|primary|active|connect|opps|activity|email|created |\n",
    "    #    For 'Privileged' rows we set tag = 'PRIV' to bubble   |\n",
    "    #    them out of comparisons (they are auto‑kept).         |\n",
    "    # ---------------------------------------------------------\n",
    "    df[\"hier_tag\"] = (\n",
    "        df[\"primary_bit\"].astype(str)   + \"|\" +\n",
    "        df[\"active_bit\"].astype(str)    + \"|\" +\n",
    "        df[\"connect_tier\"]              + \"|\" +\n",
    "        df[\"opps_bucket\"]               + \"|\" +\n",
    "        df[\"activity_tier\"]             + \"|\" +\n",
    "        df[\"email_bit\"].astype(str)     + \"|\" +\n",
    "        df[\"created_rank\"]\n",
    "    )\n",
    "    \n",
    "    # overwrite with sentinel for Owner/Admin rows\n",
    "    df.loc[df[\"is_privileged\"], \"hier_tag\"] = \"PRIV\"\n",
    "    \n",
    "    # Tidy up helper columns if desired\n",
    "    df.drop(columns=[\n",
    "        \"Admin Role_std\", \"primary_bit\", \"active_bit\", \"connect_raw\",\n",
    "        \"connect_tier\", \"opps_bucket\", \"activity_tier\", \"email_bit\",\n",
    "        \"created_rank\"\n",
    "    ], inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a87cfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 4 – Duplicate‑Candidate Generation\n",
    "--------------------------------------\n",
    "Goal:  within each Account Name group, discover records that likely\n",
    "       refer to the same person by layering fast “blocking” keys\n",
    "       (exact matches) and slower fuzzy comparisons (≈ matches).\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "• Adds `dupe_cluster_id`  — ID shared by rows judged to be duplicates\n",
    "• Keeps privileged Owner/Admin rows in scope (they can still collide)\n",
    "• No row is dropped; downstream logic decides which one to keep.\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "rapidfuzz  (pip install rapidfuzz)\n",
    "pandas\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, distance\n",
    "from itertools import combinations\n",
    "\n",
    "def add_duplicate_cluster_ids(df_in: pd.DataFrame,\n",
    "                              name_sim_threshold: int = 95,\n",
    "                              email_edit_distance: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tighter duplicate‑candidate generation:\n",
    "      • Domain‑anchored email fuzzy matching\n",
    "      • Stricter name matching (token_sort_ratio ≥ 95, length diff ≤ 2)\n",
    "    \"\"\"\n",
    "    df = _prep_normalised_fields(df_in)   # adds email_norm, name_norm, sfi_key\n",
    "    uf = UnionFind()\n",
    "\n",
    "    for acct, grp in df.groupby(\"Account Name\"):\n",
    "        idxs = grp.index.tolist()\n",
    "\n",
    "        # 1) exact‑email blocking (unchanged)\n",
    "        for _, eb in grp.groupby(\"email_norm\"):\n",
    "            ids = list(eb.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 2) surname‑first‑initial blocking (unchanged)\n",
    "        for _, sb in grp.groupby(\"sfi_key\"):\n",
    "            ids = list(sb.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 3) domain‑anchored near‑email blocking\n",
    "        #    only union if local-parts are within distance AND same domain\n",
    "        for domain, dg in grp.groupby(grp[\"email_norm\"].str.split(\"@\").str[1].fillna(\"\")):\n",
    "            ids = list(dg.index)\n",
    "            # compare every pair within this domain\n",
    "            for i, j in combinations(ids, 2):\n",
    "                local_i = df.at[i, \"email_norm\"].split(\"@\")[0]\n",
    "                local_j = df.at[j, \"email_norm\"].split(\"@\")[0]\n",
    "                if distance.Levenshtein.distance(local_i, local_j) <= email_edit_distance:\n",
    "                    uf.union(i, j)\n",
    "\n",
    "        # 4) targeted fuzzy checks for any remaining pairs\n",
    "        for i, j in combinations(idxs, 2):\n",
    "            if uf.find(i) == uf.find(j):\n",
    "                continue\n",
    "\n",
    "            e1, e2 = df.at[i, \"email_norm\"], df.at[j, \"email_norm\"]\n",
    "            # – only fuzzy‑merge on email if domains match\n",
    "            dom1 = e1.split(\"@\")[1] if \"@\" in e1 else \"\"\n",
    "            dom2 = e2.split(\"@\")[1] if \"@\" in e2 else \"\"\n",
    "            if dom1 == dom2:\n",
    "                local1, local2 = e1.split(\"@\")[0], e2.split(\"@\")[0]\n",
    "                if distance.Levenshtein.distance(local1, local2) <= email_edit_distance:\n",
    "                    uf.union(i, j)\n",
    "                    continue\n",
    "\n",
    "            # – stricter name matching: order‑aware + length guard\n",
    "            n1, n2 = df.at[i, \"name_norm\"], df.at[j, \"name_norm\"]\n",
    "            if (abs(len(n1) - len(n2)) <= 2 and\n",
    "                fuzz.token_sort_ratio(n1, n2) >= name_sim_threshold):\n",
    "                uf.union(i, j)\n",
    "\n",
    "    # 5) cluster ID assignment (unchanged)\n",
    "    root_to_cluster = {}\n",
    "    counter = 1\n",
    "    clusters = []\n",
    "    for idx in df.index:\n",
    "        r = uf.find(idx)\n",
    "        if r not in root_to_cluster:\n",
    "            root_to_cluster[r] = f\"C{counter:05d}\"\n",
    "            counter += 1\n",
    "        clusters.append(root_to_cluster[r])\n",
    "    df[\"dupe_cluster_id\"] = clusters\n",
    "\n",
    "    # clean up\n",
    "    df.drop(columns=[\"email_norm\", \"name_norm\", \"sfi_key\"], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf560b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_contacts(\"../data/duplicate_contacts_small.xlsx\")   # Step 1\n",
    "df = add_comparison_tag(df)                                # Step 3b\n",
    "df = add_duplicate_cluster_ids(df)                         # Step 4   ← here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fa37db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 5 – Canonical selection & merge mapping\n",
    "-------------------------------------------\n",
    "Annotates the DataFrame with canonical flags, merge targets, and\n",
    "resolution status, following the decision ladder described above.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def assign_canonical_records(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a copy of df_in with canonical/merge annotations (fixed version).\"\"\"\n",
    "    \n",
    "    df = df_in.copy()\n",
    "    df[\"is_canonical\"]          = False\n",
    "    df[\"canonical_contact_id\"]  = None\n",
    "    df[\"resolution_status\"]     = None\n",
    "    \n",
    "    # helper: pick earliest Created Date; if tie, lowest DataFrame index\n",
    "    def _pick_primary(rows: pd.DataFrame):\n",
    "        # Sort by Created Date, then by actual index (implicit tie‑breaker)\n",
    "        return rows.sort_values(\"Created Date\").iloc[0]\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    for cid, sub_idx in df.groupby(\"dupe_cluster_id\").groups.items():\n",
    "        sub = df.loc[sub_idx]\n",
    "        n_rows = len(sub)\n",
    "        \n",
    "        # — 0) singleton\n",
    "        if n_rows == 1:\n",
    "            idx = sub.index[0]\n",
    "            df.at[idx, \"is_canonical\"]         = True\n",
    "            df.at[idx, \"canonical_contact_id\"] = df.at[idx, \"Contact Id\"]\n",
    "            df.at[idx, \"resolution_status\"]    = \"single_record\"\n",
    "            continue\n",
    "        \n",
    "        # — 1) privileged siphon\n",
    "        priv_rows = sub[sub[\"is_privileged\"]]\n",
    "        non_priv  = sub[~sub[\"is_privileged\"]]\n",
    "        \n",
    "        if not priv_rows.empty:\n",
    "            primary_priv = _pick_primary(priv_rows)\n",
    "            prim_id      = primary_priv[\"Contact Id\"]\n",
    "            \n",
    "            # mark privileged rows\n",
    "            df.loc[priv_rows.index, \"is_canonical\"]         = True\n",
    "            df.loc[priv_rows.index, \"canonical_contact_id\"] = priv_rows[\"Contact Id\"]\n",
    "            df.loc[priv_rows.index, \"resolution_status\"]    = \"keep_privileged\"\n",
    "            \n",
    "            # merge others into the primary privileged row\n",
    "            df.loc[non_priv.index, \"is_canonical\"]         = False\n",
    "            df.loc[non_priv.index, \"canonical_contact_id\"] = prim_id\n",
    "            df.loc[non_priv.index, \"resolution_status\"]    = \"merge_into_privileged\"\n",
    "            continue\n",
    "        \n",
    "        # — 2) no privileged rows\n",
    "        sorted_sub = sub.sort_values(\"hier_tag\", ascending=False)\n",
    "        top_tag    = sorted_sub.iloc[0][\"hier_tag\"]\n",
    "        top_rows   = sorted_sub[sorted_sub[\"hier_tag\"] == top_tag]\n",
    "        \n",
    "        if len(top_rows) == 1:\n",
    "            winner_idx = top_rows.index[0]\n",
    "            win_id     = top_rows.iloc[0][\"Contact Id\"]\n",
    "            \n",
    "            df.at[winner_idx, \"is_canonical\"]         = True\n",
    "            df.at[winner_idx, \"canonical_contact_id\"] = win_id\n",
    "            df.at[winner_idx, \"resolution_status\"]    = \"keep\"\n",
    "            \n",
    "            losers_idx = sorted_sub.index.difference([winner_idx])\n",
    "            df.loc[losers_idx, \"is_canonical\"]         = False\n",
    "            df.loc[losers_idx, \"canonical_contact_id\"] = win_id\n",
    "            df.loc[losers_idx, \"resolution_status\"]    = \"merge\"\n",
    "        else:\n",
    "            tie_indices   = top_rows.index\n",
    "            first_tie_id  = top_rows.iloc[0][\"Contact Id\"]\n",
    "            \n",
    "            df.loc[tie_indices, \"is_canonical\"]          = True\n",
    "            df.loc[tie_indices, \"canonical_contact_id\"]  = tie_indices.map(df[\"Contact Id\"])\n",
    "            df.loc[tie_indices, \"resolution_status\"]     = \"needs_review\"\n",
    "            \n",
    "            rest = sorted_sub.index.difference(tie_indices)\n",
    "            df.loc[rest, \"is_canonical\"]         = False\n",
    "            df.loc[rest, \"canonical_contact_id\"] = first_tie_id\n",
    "            df.loc[rest, \"resolution_status\"]    = \"merge\"\n",
    "    \n",
    "    df[\"canonical_contact_id\"] = df[\"canonical_contact_id\"].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29b6f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 6 – Export workbook + merge log\n",
    "------------------------------------\n",
    "Takes the DataFrame returned by STEP 5 and writes a single Excel file\n",
    "with three sheets:\n",
    "  • master_contacts\n",
    "  • change_log\n",
    "  • needs_review\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def export_dedupe_results(df_in: pd.DataFrame,\n",
    "                          out_path: str | Path = \"output/deduped_contacts.xlsx\"):\n",
    "    \"\"\"\n",
    "    Write *df_in* to a multi‑sheet Excel workbook summarising the dedupe run.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_in : pd.DataFrame\n",
    "        Contact table after STEP 5 (has canonical_contact_id, resolution_status, etc.).\n",
    "    out_path : str or pathlib.Path, optional\n",
    "        Where to write the workbook.\n",
    "    \"\"\"\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)     # create output/ dir\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # 1) Master sheet  – ready for CRM import\n",
    "    # -----------------------------------------\n",
    "    master_cols = [\n",
    "        \"Account Name\", \"Full Name\", \"Email\", \"Contact Id\",\n",
    "        \"canonical_contact_id\", \"resolution_status\",\n",
    "        \"dupe_cluster_id\", \"is_canonical\", \"hier_tag\"\n",
    "    ]\n",
    "    master_sheet = df_in[master_cols].sort_values(\n",
    "        [\"dupe_cluster_id\", \"is_canonical\"], ascending=[True, False]\n",
    "    )\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # 2) Change‑log  – one row per merged record\n",
    "    # -----------------------------------------\n",
    "    merge_mask = df_in[\"resolution_status\"].isin(\n",
    "        [\"merge\", \"merge_into_privileged\"]\n",
    "    )\n",
    "    change_log = (\n",
    "        df_in.loc[merge_mask,\n",
    "                  [\"dupe_cluster_id\", \"Contact Id\",\n",
    "                   \"canonical_contact_id\", \"resolution_status\",\n",
    "                   \"hier_tag\"]]\n",
    "          .rename(columns={\"Contact Id\": \"old_contact_id\"})\n",
    "          .sort_values(\"dupe_cluster_id\")\n",
    "    )\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # 3) Needs‑review sheet\n",
    "    # -----------------------------------------\n",
    "    review_sheet = df_in[df_in[\"resolution_status\"] == \"needs_review\"] \\\n",
    "                     .sort_values(\"dupe_cluster_id\")\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # 4) Write to Excel (openpyxl engine)\n",
    "    # -----------------------------------------\n",
    "    with pd.ExcelWriter(out_path, engine=\"openpyxl\") as xl:\n",
    "        master_sheet.to_excel(xl, sheet_name=\"master_contacts\", index=False)\n",
    "        change_log.to_excel(xl, sheet_name=\"change_log\",     index=False)\n",
    "        review_sheet.to_excel(xl, sheet_name=\"needs_review\", index=False)\n",
    "    \n",
    "    print(f\"✅  Dedupe workbook written to {out_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3c04d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import distance\n",
    "\n",
    "def apply_email_merge_or_inactivate(df: pd.DataFrame,\n",
    "                                    max_email_dist: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each non-canonical row:\n",
    "      • If any canonical share the same email domain AND\n",
    "        their local-part is within `max_email_dist`, merge into the\n",
    "        best one (highest hier_tag).  \n",
    "      • Otherwise mark resolution_status = \"inactive\".\n",
    "\n",
    "    Assumes df has:\n",
    "      • email_norm       (lowercased, stripped full email)\n",
    "      • dupe_cluster_id\n",
    "      • is_canonical\n",
    "      • hier_tag\n",
    "      • Contact Id\n",
    "      • resolution_status (from Step 5)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Build map: cluster_id -> list of canonical candidates\n",
    "    can_lookup: dict[str, list[tuple[int,str,str,str,str]]] = {}\n",
    "    canon = df[df[\"is_canonical\"]]\n",
    "    for cid, sub in canon.groupby(\"dupe_cluster_id\"):\n",
    "        # each tuple: (true_idx, email_norm, hier_tag, Contact Id, resolution_status)\n",
    "        can_lookup[cid] = [\n",
    "            (idx, sub.at[idx, \"email_norm\"], sub.at[idx, \"hier_tag\"],\n",
    "                   sub.at[idx, \"Contact Id\"], sub.at[idx, \"resolution_status\"])\n",
    "            for idx in sub.index\n",
    "        ]\n",
    "\n",
    "    # 2) Process every non-canonical row\n",
    "    for idx, row in df[~df[\"is_canonical\"]].iterrows():\n",
    "        cid        = row[\"dupe_cluster_id\"]\n",
    "        my_email   = row[\"email_norm\"] or \"\"\n",
    "        domain, *local = my_email.split(\"@\")\n",
    "        local = local[0] if local else \"\"\n",
    "\n",
    "        best_match = None  # will hold (can_idx, hier_tag, can_cid, can_status)\n",
    "        for can_idx, can_email, can_tag, can_cid, can_status in can_lookup.get(cid, []):\n",
    "            # split candidate\n",
    "            dom2, *loc2 = can_email.split(\"@\")\n",
    "            loc2 = loc2[0] if loc2 else \"\"\n",
    "            # only compare if domains are exact\n",
    "            if domain == dom2:\n",
    "                if distance.Levenshtein.distance(local, loc2) <= max_email_dist:\n",
    "                    # is a valid match\n",
    "                    # pick the one with higher hier_tag\n",
    "                    if (best_match is None) or (can_tag > best_match[1]):\n",
    "                        best_match = (can_idx, can_tag, can_cid, can_status)\n",
    "\n",
    "        if best_match:\n",
    "            _, _, target_cid, target_status = best_match\n",
    "            df.at[idx, \"canonical_contact_id\"] = target_cid\n",
    "            # preserve privileged semantics\n",
    "            df.at[idx, \"resolution_status\"] = (\n",
    "                \"merge_into_privileged\"\n",
    "                if target_status == \"keep_privileged\"\n",
    "                else \"merge\"\n",
    "            )\n",
    "        else:\n",
    "            # no close‑enough email → inactivate\n",
    "            df.at[idx, \"canonical_contact_id\"] = None\n",
    "            df.at[idx, \"resolution_status\"]       = \"inactive\"\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "947ca657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Dedupe workbook written to C:\\Users\\Elioa\\OneDrive\\Projects\\data-dedup-pilot\\notebooks\\output\\deduped_contacts.xlsx\n"
     ]
    }
   ],
   "source": [
    "df = load_contacts(\"../data/duplicate_contacts_small.xlsx\")   # Step 1\n",
    "df = add_comparison_tag(df)                                # Step 3b\n",
    "df = add_duplicate_cluster_ids(df)                         # Step 4\n",
    "df = assign_canonical_records(df)                          # Step 5\n",
    "export_dedupe_results(df, \"output/deduped_contacts.xlsx\")  # Step 6\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
