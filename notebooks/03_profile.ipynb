{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5e098d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 0 – Set API key for OpenAI calls\n",
    "import os\n",
    "# Store your OpenAI secret so the client can pick it up\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7477904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% Cell 1 – Imports, constants, and client setup\n",
    "import logging                              # Standard Python logging\n",
    "import pandas as pd                         # DataFrames & Excel I/O\n",
    "from pathlib import Path                    # Filesystem paths\n",
    "from datetime import datetime               # Working with dates\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from difflib import SequenceMatcher         # Name‐similarity metric\n",
    "from itertools import combinations          # Pairwise loops for clustering\n",
    "from openai import OpenAI, OpenAIError      # OpenAI v1 client & errors\n",
    "from rapidfuzz import fuzz, distance\n",
    "\n",
    "# Instantiate the OpenAI client (will read OPENAI_API_KEY)\n",
    "client = OpenAI()\n",
    "\n",
    "# Turn on INFO‑level logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Map ConnectLink statuses to sorting tiers (string so tags sort lexicographically)\n",
    "CONNECT_TIER = {\"A\": \"3\", \"I\": \"2\", \"U\": \"2\", \"\": \"1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "587aa3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1 – Ingestion for your exact headers\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_contacts(file_path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Read the sheet as-is.\n",
    "    2) Rename exactly your 14 incoming headers to the 12 the pipeline expects.\n",
    "    3) Synthesize Full Name from First + Last Name.\n",
    "    4) Validate the required set.\n",
    "    5) Normalize only those required fields; preserve all others.\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    # 1) Read everything (dates as strings for now)\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\", dtype=str)\n",
    "\n",
    "    # 2) Exact rename map\n",
    "    rename_map = {\n",
    "        \"Account Name: Acct_ID_18\": \"Account Name\",\n",
    "        \"Contact_id_18\":            \"Contact Id\",\n",
    "        \"Primary Contact Any\":      \"Primary Contact\",\n",
    "        \"Agile Contact Email\":      \"Connect Link Email\",\n",
    "        \"# of Cases\":               \"# of cases\",\n",
    "        \"# of Opps\":                \"# of opps\"\n",
    "        # Leave \"First Name\",\"Last Name\",\"Email\",\"Admin Role\",\"Active Contact\",\n",
    "        # \"ConnectLink Status\",\"Last Activity\",\"Created Date\" unchanged.\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # 3) Build Full Name if missing\n",
    "    if \"Full Name\" not in df.columns:\n",
    "        if {\"First Name\",\"Last Name\"}.issubset(df.columns):\n",
    "            df[\"Full Name\"] = (\n",
    "                df[\"First Name\"].fillna(\"\").str.strip() + \" \" +\n",
    "                df[\"Last Name\"].fillna(\"\").str.strip()\n",
    "            ).str.strip()\n",
    "        else:\n",
    "            raise ValueError(\"Missing both Full Name and First Name + Last Name\")\n",
    "\n",
    "    # 4) Coerce the two date columns, if present\n",
    "    for dt in [\"Last Activity\",\"Created Date\"]:\n",
    "        if dt in df.columns:\n",
    "            df[dt] = pd.to_datetime(df[dt], errors=\"coerce\")\n",
    "\n",
    "    # 5) Validate the canonical 12 exist\n",
    "    required = [\n",
    "        \"Account Name\", \"Full Name\", \"Email\", \"Contact Id\",\n",
    "        \"Admin Role\", \"Primary Contact\", \"Active Contact\",\n",
    "        \"ConnectLink Status\", \"Connect Link Email\",\n",
    "        \"# of cases\", \"# of opps\", \"Last Activity\", \"Created Date\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # 6) Normalize only those 12:\n",
    "    #    - Fill text→\"\" and trim spaces\n",
    "    #    - Map Primary Contact → boolean\n",
    "    for c in required:\n",
    "        if df[c].dtype == object:\n",
    "            df[c] = (\n",
    "                df[c]\n",
    "                  .fillna(\"\")\n",
    "                  .astype(str)\n",
    "                  .str.strip()\n",
    "                  .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            )\n",
    "    df[\"Primary Contact\"] = df[\"Primary Contact\"].map(\n",
    "        lambda x: str(x).lower() in {\"true\",\"1\",\"yes\"}\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "33d4e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2 – Revised STEP 2: build hier_tag with combined email bit, blank‐dates=best\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def add_comparison_tag(df: pd.DataFrame, today: datetime|None=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds:\n",
    "      • is_privileged (owner/admin)\n",
    "      • hier_tag: primary|active|connect|opps|activity|email|created\n",
    "    Notes:\n",
    "      - activity tier treats blank Last Activity as most‐recent (tier “1”)\n",
    "      - email bit = 1 if either Email or Connect Link Email is present\n",
    "    \"\"\"\n",
    "    if today is None:\n",
    "        today = pd.Timestamp.today().normalize()\n",
    "    df = df.copy()\n",
    "\n",
    "    # ensure dates are datetime\n",
    "    df[\"Last Activity\"] = pd.to_datetime(df[\"Last Activity\"], errors=\"coerce\")\n",
    "    df[\"Created Date\"]  = pd.to_datetime(df[\"Created Date\"],  errors=\"coerce\")\n",
    "\n",
    "    # privileged\n",
    "    df[\"is_privileged\"] = df[\"Admin Role\"].str.lower().str.strip().isin({\"owner\",\"admin\"})\n",
    "\n",
    "    # bits\n",
    "    df[\"primary_bit\"] = df[\"Primary Contact\"].astype(bool).astype(int)\n",
    "    df[\"active_bit\"]  = (df[\"Active Contact\"].str.lower().str.strip() == \"active\").astype(int)\n",
    "    df[\"connect_tier\"]= df[\"ConnectLink Status\"].str.upper().str.strip().map(CONNECT_TIER).fillna(\"1\")\n",
    "\n",
    "    # opps bucket\n",
    "    opps = df[\"# of opps\"].fillna(0).astype(int)\n",
    "    df[\"opps_bucket\"] = pd.cut(opps, [-1,0,3,float(\"inf\")], labels=[\"Z\",\"L\",\"H\"]).astype(str)\n",
    "\n",
    "    # activity tier (blank→tier 1)\n",
    "    days = (today - df[\"Last Activity\"]).dt.days\n",
    "    df[\"activity_tier\"] = pd.cut(days, [-float(\"inf\"),365,912,float(\"inf\")], labels=[\"1\",\"2\",\"3\"]).astype(str)\n",
    "    df.loc[days.isna(), \"activity_tier\"] = \"1\"\n",
    "\n",
    "    # U-tier demotion\n",
    "    mask_demote = (\n",
    "        (df[\"ConnectLink Status\"].str.upper()==\"U\") &\n",
    "        (df[\"opps_bucket\"]==\"Z\") &\n",
    "        (df[\"activity_tier\"].isin({\"3\"}))\n",
    "    )\n",
    "    df.loc[mask_demote, \"connect_tier\"] = \"1\"\n",
    "\n",
    "    # combined email presence bit\n",
    "    df[\"email_bit\"] = (\n",
    "        df[\"Email\"].astype(str).str.strip().ne(\"\") |\n",
    "        df[\"Connect Link Email\"].astype(str).str.strip().ne(\"\")\n",
    "    ).astype(int)\n",
    "\n",
    "    # created rank\n",
    "    crdays = (today - df[\"Created Date\"]).dt.days.fillna(0).clip(0,99999).astype(int)\n",
    "    df[\"created_rank\"] = crdays.astype(str).str.zfill(5)\n",
    "\n",
    "    # compose tag\n",
    "    df[\"hier_tag\"] = (\n",
    "        df[\"primary_bit\"].astype(str)+\"|\"+\n",
    "        df[\"active_bit\"].astype(str)+\"|\"+\n",
    "        df[\"connect_tier\"]+\"|\"+\n",
    "        df[\"opps_bucket\"]+\"|\"+\n",
    "        df[\"activity_tier\"]+\"|\"+\n",
    "        df[\"email_bit\"].astype(str)+\"|\"+\n",
    "        df[\"created_rank\"]\n",
    "    )\n",
    "\n",
    "    # privileged override\n",
    "    df.loc[df[\"is_privileged\"], \"hier_tag\"] = \"PRIV\"\n",
    "\n",
    "    return df.drop(columns=[\n",
    "        \"primary_bit\",\"active_bit\",\"connect_tier\",\n",
    "        \"opps_bucket\",\"activity_tier\",\"email_bit\",\"created_rank\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5130c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell X – Helpers for clustering (override your old _prep_normalised_fields)\n",
    "import pandas as pd\n",
    "\n",
    "def _prep_normalised_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add exactly these helper columns:\n",
    "      • email_norm   – primary Email lower/trimmed\n",
    "      • connect_norm – Agile/ConnectLink Email lower/trimmed\n",
    "      • name_norm    – Full Name lower, letters+spaces only\n",
    "      • sfi_key      – surname + '_' + first initial\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # 1) primary email normalized\n",
    "    df[\"email_norm\"]   = df[\"Email\"].astype(str).str.lower().str.strip()\n",
    "    # 2) agile/contact‐link email normalized\n",
    "    df[\"connect_norm\"] = df[\"Connect Link Email\"].astype(str).str.lower().str.strip()\n",
    "    # 3) full name cleaned\n",
    "    df[\"name_norm\"]    = (\n",
    "        df[\"Full Name\"]\n",
    "          .astype(str)\n",
    "          .str.lower()\n",
    "          .str.replace(r\"[^a-z ]\",\"\",regex=True)\n",
    "          .str.strip()\n",
    "    )\n",
    "    # 4) surname+initial blocking key\n",
    "    def make_sfi(s: str) -> str:\n",
    "        parts = s.split()\n",
    "        return f\"{parts[-1]}_{parts[0][0]}\" if len(parts) >= 2 else \"\"\n",
    "    df[\"sfi_key\"]      = df[\"name_norm\"].apply(make_sfi)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell Y – Revised STEP 4: duplicate clustering in your exact order\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, distance\n",
    "from itertools import combinations\n",
    "\n",
    "def add_duplicate_cluster_ids(\n",
    "    df_in: pd.DataFrame,\n",
    "    name_sim_threshold: int = 95,\n",
    "    email_edit_distance: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Block & fuzzy‐match in this sequence:\n",
    "      1) exact primary email\n",
    "      2) exact agile email\n",
    "      3) exact full name\n",
    "      4) surname‐first‐initial\n",
    "      5) one‐char off local‐parts (same domain)\n",
    "      6) fuzzy name (token_sort_ratio)\n",
    "    \"\"\"\n",
    "    df = _prep_normalised_fields(df_in)\n",
    "    uf = UnionFind()\n",
    "\n",
    "    for acct, grp in df.groupby(\"Account Name\"):\n",
    "        idxs = list(grp.index)\n",
    "\n",
    "        # 1) exact primary email\n",
    "        for _, block in grp.groupby(\"email_norm\"):\n",
    "            ids = list(block.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 2) exact agile email\n",
    "        for _, block in grp.groupby(\"connect_norm\"):\n",
    "            ids = list(block.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 3) exact full name\n",
    "        for _, block in grp.groupby(\"name_norm\"):\n",
    "            ids = list(block.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 4) surname‐first‐initial\n",
    "        for _, block in grp.groupby(\"sfi_key\"):\n",
    "            ids = list(block.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 5) one‐char off email local‐part\n",
    "        for domain, sub in grp.groupby(grp[\"email_norm\"].str.split(\"@\").str[1].fillna(\"\")):\n",
    "            ids = list(sub.index)\n",
    "            for i, j in combinations(ids, 2):\n",
    "                local_i = df.at[i, \"email_norm\"].split(\"@\")[0]\n",
    "                local_j = df.at[j, \"email_norm\"].split(\"@\")[0]\n",
    "                if distance.Levenshtein.distance(local_i, local_j) <= email_edit_distance:\n",
    "                    uf.union(i, j)\n",
    "\n",
    "        # 6) fuzzy full name\n",
    "        for i, j in combinations(idxs, 2):\n",
    "            if uf.find(i) == uf.find(j):\n",
    "                continue\n",
    "            n1, n2 = df.at[i, \"name_norm\"], df.at[j, \"name_norm\"]\n",
    "            if fuzz.token_sort_ratio(n1, n2) >= name_sim_threshold:\n",
    "                uf.union(i, j)\n",
    "\n",
    "    # assign stable cluster IDs\n",
    "    root2cid = {}\n",
    "    clusters = []\n",
    "    counter = 1\n",
    "    for i in df.index:\n",
    "        root = uf.find(i)\n",
    "        if root not in root2cid:\n",
    "            root2cid[root] = f\"C{counter:05d}\"\n",
    "            counter += 1\n",
    "        clusters.append(root2cid[root])\n",
    "\n",
    "    df_in[\"dupe_cluster_id\"] = clusters\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "90ab2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_canonical_records(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    PASS 1: same name_norm + >1 distinct email_norm → keep only owners, inactivate others\n",
    "    PASS 2: existing dupe_cluster_id logic (email‐dedup, privileged, singleton, hier_tag)\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    # normalize emails & names\n",
    "    df[\"email_norm\"] = (\n",
    "        df[\"Email\"].astype(str)\n",
    "                .str.lower()\n",
    "                .str.strip()\n",
    "    )\n",
    "    # ensure name_norm exists\n",
    "    df[\"name_norm\"] = (\n",
    "        df[\"Full Name\"].astype(str)\n",
    "                      .str.lower()\n",
    "                      .str.replace(r\"[^a-z ]\", \"\", regex=True)\n",
    "                      .str.strip()\n",
    "    )\n",
    "    # init output cols\n",
    "    df[\"is_canonical\"]         = False\n",
    "    df[\"canonical_contact_id\"] = None\n",
    "    df[\"resolution_status\"]    = None\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 1: same-name, different-email\n",
    "    # -----------------------------\n",
    "    for acct, acct_idxs in df.groupby(\"Account Name\").groups.items():\n",
    "        subset = df.loc[acct_idxs]\n",
    "        for nm, name_idxs in subset.groupby(\"name_norm\").groups.items():\n",
    "            if len(name_idxs) <= 1:\n",
    "                continue\n",
    "            distinct_emails = df.loc[name_idxs, \"email_norm\"].nunique()\n",
    "            if distinct_emails > 1:\n",
    "                # find privileged rows for this name\n",
    "                priv = df.loc[name_idxs][df.loc[name_idxs, \"is_privileged\"]].index.tolist()\n",
    "                nonpriv = [i for i in name_idxs if i not in priv]\n",
    "                # keep privileged\n",
    "                for i in priv:\n",
    "                    df.at[i, \"is_canonical\"] = True\n",
    "                    df.at[i, \"canonical_contact_id\"] = df.at[i, \"Contact Id\"]\n",
    "                    df.at[i, \"resolution_status\"] = \"keep_privileged\"\n",
    "                # inactivate the rest\n",
    "                for i in nonpriv:\n",
    "                    df.at[i, \"is_canonical\"] = False\n",
    "                    df.at[i, \"canonical_contact_id\"] = None\n",
    "                    df.at[i, \"resolution_status\"] = \"inactive\"\n",
    "\n",
    "    # helper for picking earliest created\n",
    "    def pick_earliest(rows: pd.DataFrame) -> pd.Series:\n",
    "        return rows.sort_values(\"Created Date\").iloc[0]\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 2: original dupe_cluster_id logic\n",
    "    # -----------------------------\n",
    "    for cluster_id, grp in df.groupby(\"dupe_cluster_id\"):\n",
    "        # only those not yet resolved\n",
    "        remaining = grp[grp[\"resolution_status\"].isna()]\n",
    "        if remaining.empty:\n",
    "            continue\n",
    "\n",
    "        sub = remaining.copy()\n",
    "\n",
    "        # 1) PRE‑PASS: dedupe non‑priv by email_norm\n",
    "        non_priv = sub[~sub[\"is_privileged\"]]\n",
    "        for email, block in non_priv.groupby(\"email_norm\"):\n",
    "            if email and len(block) > 1:\n",
    "                primary = pick_earliest(block)\n",
    "                pid = primary[\"Contact Id\"]\n",
    "\n",
    "                # keep that one\n",
    "                df.at[primary.name, \"is_canonical\"] = True\n",
    "                df.at[primary.name, \"canonical_contact_id\"] = pid\n",
    "                df.at[primary.name, \"resolution_status\"] = \"keep_email_dedup\"\n",
    "\n",
    "                # merge the rest\n",
    "                others = block.index.difference([primary.name])\n",
    "                for i in others:\n",
    "                    df.at[i, \"is_canonical\"] = False\n",
    "                    df.at[i, \"canonical_contact_id\"] = pid\n",
    "                    df.at[i, \"resolution_status\"] = \"merge_email_dedup\"\n",
    "\n",
    "        # refresh remaining\n",
    "        done = df.loc[grp.index][df.loc[grp.index, \"resolution_status\"].notna()].index\n",
    "        to_do = grp.index.difference(done)\n",
    "        if to_do.empty:\n",
    "            continue\n",
    "        sub = df.loc[to_do]\n",
    "\n",
    "        # 2) Privileged siphon\n",
    "        priv = sub[sub[\"is_privileged\"]]\n",
    "        if not priv.empty:\n",
    "            primary = pick_earliest(priv)\n",
    "            pid = primary[\"Contact Id\"]\n",
    "\n",
    "            # keep all privileged\n",
    "            for i in priv.index:\n",
    "                df.at[i, \"is_canonical\"] = True\n",
    "                df.at[i, \"canonical_contact_id\"] = df.at[i, \"Contact Id\"]\n",
    "                df.at[i, \"resolution_status\"] = \"keep_privileged\"\n",
    "\n",
    "            # merge others into that earliest privileged\n",
    "            others = sub.index.difference(priv.index)\n",
    "            for i in others:\n",
    "                df.at[i, \"is_canonical\"] = False\n",
    "                df.at[i, \"canonical_contact_id\"] = pid\n",
    "                df.at[i, \"resolution_status\"] = \"merge_into_privileged\"\n",
    "            continue\n",
    "\n",
    "        # 3) Singleton\n",
    "        if len(sub) == 1:\n",
    "            i = sub.index[0]\n",
    "            df.at[i, \"is_canonical\"] = True\n",
    "            df.at[i, \"canonical_contact_id\"] = df.at[i, \"Contact Id\"]\n",
    "            df.at[i, \"resolution_status\"] = \"single_record\"\n",
    "            continue\n",
    "\n",
    "        # 4) hier_tag competition\n",
    "        sorted_sub = sub.sort_values(\"hier_tag\", ascending=False)\n",
    "        top_tag = sorted_sub.iloc[0][\"hier_tag\"]\n",
    "        tied    = sorted_sub[sorted_sub[\"hier_tag\"] == top_tag]\n",
    "\n",
    "        if len(tied) == 1:\n",
    "            winner = tied.index[0]\n",
    "            win_cid = tied.iloc[0][\"Contact Id\"]\n",
    "            # keep winner\n",
    "            df.at[winner, \"is_canonical\"] = True\n",
    "            df.at[winner, \"canonical_contact_id\"] = win_cid\n",
    "            df.at[winner, \"resolution_status\"] = \"keep\"\n",
    "            # merge losers\n",
    "            losers = sorted_sub.index.difference([winner])\n",
    "            for i in losers:\n",
    "                df.at[i, \"is_canonical\"] = False\n",
    "                df.at[i, \"canonical_contact_id\"] = win_cid\n",
    "                df.at[i, \"resolution_status\"] = \"merge\"\n",
    "        else:\n",
    "            # tie → keep all tied\n",
    "            for i in tied.index:\n",
    "                df.at[i, \"is_canonical\"] = True\n",
    "                df.at[i, \"canonical_contact_id\"] = df.at[i, \"Contact Id\"]\n",
    "                df.at[i, \"resolution_status\"] = \"keep_tie\"\n",
    "            # merge the rest into the first tied\n",
    "            first_cid = tied.iloc[0][\"Contact Id\"]\n",
    "            rest = sorted_sub.index.difference(tied.index)\n",
    "            for i in rest:\n",
    "                df.at[i, \"is_canonical\"] = False\n",
    "                df.at[i, \"canonical_contact_id\"] = first_cid\n",
    "                df.at[i, \"resolution_status\"] = \"merge\"\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f37541d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7 – Simplified STEP 5a: allow any one‑char email diffs\n",
    "def apply_email_merge_or_inactivate(df: pd.DataFrame,\n",
    "                                    max_email_dist: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge rules for non-canonical rows:\n",
    "      1) blank email anywhere → wildcard merge\n",
    "      2) exact match         → merge\n",
    "      3) Levenshtein ≤ 1     → merge (no forbidden‐index checks)\n",
    "      4) else                → inactive\n",
    "\n",
    "    Owner/Admin (is_privileged) always stay canonical.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 0) Protect privileged rows\n",
    "    priv_mask = df[\"is_privileged\"].fillna(False)\n",
    "    df.loc[priv_mask, \"is_canonical\"]         = True\n",
    "    df.loc[priv_mask, \"resolution_status\"]    = \"keep_privileged\"\n",
    "    df.loc[priv_mask, \"canonical_contact_id\"] = df.loc[priv_mask, \"Contact Id\"]\n",
    "\n",
    "    # normalize email\n",
    "    df[\"email_norm\"] = df[\"Email\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # target only non‑canonical, non‑privileged\n",
    "    mask_nc = (~df[\"is_canonical\"].fillna(False)) & (~priv_mask)\n",
    "    df.loc[mask_nc, [\"resolution_status\",\"canonical_contact_id\"]] = [None, None]\n",
    "\n",
    "    # build lookup of canonicals per cluster\n",
    "    can_lookup = {}\n",
    "    for cid, sub in df[df[\"is_canonical\"]].groupby(\"dupe_cluster_id\"):\n",
    "        can_lookup[cid] = [\n",
    "            (idx,\n",
    "             sub.at[idx, \"email_norm\"],\n",
    "             sub.at[idx, \"hier_tag\"],\n",
    "             sub.at[idx, \"Contact Id\"],\n",
    "             sub.at[idx, \"resolution_status\"])\n",
    "            for idx in sub.index\n",
    "        ]\n",
    "\n",
    "    # inline Levenshtein (same DP as before)\n",
    "    def levenshtein(a: str, b: str) -> int:\n",
    "        m, n = len(a), len(b)\n",
    "        if m < n:\n",
    "            return levenshtein(b, a)\n",
    "        if n == 0:\n",
    "            return m\n",
    "        prev = list(range(n+1))\n",
    "        for i, ca in enumerate(a, start=1):\n",
    "            curr = [i] + [0]*n\n",
    "            for j, cb in enumerate(b, start=1):\n",
    "                ins = curr[j-1] + 1\n",
    "                dele= prev[j]   + 1\n",
    "                rep = prev[j-1] + (ca != cb)\n",
    "                curr[j] = min(ins, dele, rep)\n",
    "            prev = curr\n",
    "        return prev[n]\n",
    "\n",
    "    # process each non‑canonical row\n",
    "    for idx, row in df[mask_nc].iterrows():\n",
    "        me = row[\"email_norm\"] or \"\"\n",
    "        candidates = can_lookup.get(row[\"dupe_cluster_id\"], [])\n",
    "        best = None\n",
    "\n",
    "        # CASE 1: blank wildcard\n",
    "        if me == \"\":\n",
    "            best = max(candidates, key=lambda x: x[2], default=None)\n",
    "        else:\n",
    "            for can_idx, ce, tag, cid_val, stat in candidates:\n",
    "                # wildcard if canonical blank\n",
    "                if ce == \"\":\n",
    "                    best = (can_idx, ce, tag, cid_val, stat)\n",
    "                    break\n",
    "                # exact match\n",
    "                if me == ce:\n",
    "                    best = (can_idx, ce, tag, cid_val, stat)\n",
    "                    break\n",
    "                # one‐char off anywhere\n",
    "                if levenshtein(me, ce) <= max_email_dist:\n",
    "                    best = (can_idx, ce, tag, cid_val, stat)\n",
    "                    break\n",
    "\n",
    "        # apply merge or inactive\n",
    "        if best:\n",
    "            _, _, _, tgt_cid, tgt_stat = best\n",
    "            df.at[idx, \"canonical_contact_id\"] = tgt_cid\n",
    "            df.at[idx, \"resolution_status\"]    = (\n",
    "                \"merge_into_privileged\" if tgt_stat == \"keep_privileged\" else \"merge\"\n",
    "            )\n",
    "        else:\n",
    "            df.at[idx, \"canonical_contact_id\"] = None\n",
    "            df.at[idx, \"resolution_status\"]    = \"inactive\"\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e553fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# STEP 6 – Export Results (preserve all original columns + add dedupe columns)\n",
    "# -----------------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def export_dedupe_results(df: pd.DataFrame,\n",
    "                          out_path: str | Path = \"output/deduped.xlsx\"):\n",
    "    \"\"\"\n",
    "    Write your full DataFrame (all original columns, in original order)\n",
    "    plus the dedupe output fields appended at the end, to 'master_contacts',\n",
    "    and also produce 'change_log' and 'needs_review' sheets.\n",
    "    \"\"\"\n",
    "    path = Path(out_path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Determine final master sheet column order:\n",
    "    #    start with all original columns, then add the new ones\n",
    "    original_cols = list(df.columns)  # preserves their current order\n",
    "    dedupe_cols = [\n",
    "        \"canonical_contact_id\",\n",
    "        \"resolution_status\",\n",
    "        \"dupe_cluster_id\",\n",
    "        \"is_canonical\",\n",
    "        \"hier_tag\"\n",
    "    ]\n",
    "    # only append those not already present\n",
    "    final_master_cols = original_cols + [c for c in dedupe_cols if c not in original_cols]\n",
    "\n",
    "    # 2) Build master sheet\n",
    "    master_sheet = df[final_master_cols].sort_values(\n",
    "        by=[\"dupe_cluster_id\", \"is_canonical\"],\n",
    "        ascending=[True, False]\n",
    "    )\n",
    "\n",
    "    # 3) Change‑log – one row per merged record\n",
    "    merge_mask = df[\"resolution_status\"].isin({\"merge\", \"merge_into_privileged\"})\n",
    "    change_log = (\n",
    "        df.loc[merge_mask, [\"dupe_cluster_id\", \"Contact Id\",\n",
    "                            \"canonical_contact_id\", \"resolution_status\", \"hier_tag\"]]\n",
    "          .rename(columns={\"Contact Id\": \"old_contact_id\"})\n",
    "    )\n",
    "\n",
    "    # 4) Needs‑review – only those flagged for manual check\n",
    "    review_sheet = df[df[\"resolution_status\"] == \"needs_review\"][final_master_cols]\n",
    "\n",
    "    # 5) Write to Excel\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as xl:\n",
    "        master_sheet.to_excel(xl, sheet_name=\"master_contacts\", index=False)\n",
    "        change_log.to_excel(xl, sheet_name=\"change_log\",     index=False)\n",
    "        review_sheet.to_excel(xl, sheet_name=\"needs_review\", index=False)\n",
    "\n",
    "    logger.info(\"Wrote dedupe workbook to %s\", path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3bf97f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9 – Invoke the pipeline\n",
    "# Simply call `main()` to run all steps\n",
    "def main():\n",
    "    df = load_contacts(\"../data/Duplicate_Contact_Scrub.xlsx\")\n",
    "    df = add_comparison_tag(df)\n",
    "    df = add_duplicate_cluster_ids(df)\n",
    "    df = assign_canonical_records(df)\n",
    "    df = apply_email_merge_or_inactivate(df)\n",
    "    export_dedupe_results(df, \"output/deduped_contacts.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "79ea0f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Wrote dedupe workbook to C:\\Users\\Elioa\\OneDrive\\Projects\\data-dedup-pilot\\notebooks\\output\\deduped_contacts.xlsx\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
