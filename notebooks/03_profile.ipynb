{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5e098d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 0 – Set API key for OpenAI calls\n",
    "import os\n",
    "# Store your OpenAI secret so the client can pick it up\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7477904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% Cell 1 – Imports, constants, and client setup\n",
    "import logging                              # Standard Python logging\n",
    "import pandas as pd                         # DataFrames & Excel I/O\n",
    "from pathlib import Path                    # Filesystem paths\n",
    "from datetime import datetime               # Working with dates\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from difflib import SequenceMatcher         # Name‐similarity metric\n",
    "from itertools import combinations          # Pairwise loops for clustering\n",
    "from openai import OpenAI, OpenAIError      # OpenAI v1 client & errors\n",
    "from rapidfuzz import fuzz, distance\n",
    "\n",
    "# Instantiate the OpenAI client (will read OPENAI_API_KEY)\n",
    "client = OpenAI()\n",
    "\n",
    "# Turn on INFO‑level logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Map ConnectLink statuses to sorting tiers (string so tags sort lexicographically)\n",
    "CONNECT_TIER = {\"A\": \"3\", \"I\": \"2\", \"U\": \"2\", \"\": \"1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "587aa3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# STEP 1 – Ingestion & Validation (with column‐name synonyms)\n",
    "# ----------------------------------------------------------------------\n",
    "# %% Cell: Revised STEP 1 – Ingestion & Flexible Rename\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_contacts(file_path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Read in the Excel/CSV.\n",
    "    2) Fuzzy‐match and rename any variant headers to our canonical set.\n",
    "    3) Validate that all required columns now exist.\n",
    "    4) Normalize only those required fields; leave extras intact.\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    # 1) Read everything in as strings, parse dates if present\n",
    "    df = pd.read_excel(\n",
    "        path,\n",
    "        engine=\"openpyxl\",\n",
    "        dtype=str,\n",
    "        parse_dates=[c for c in [\"Last Activity\",\"Created Date\"] if c in pd.read_excel(path, nrows=0).columns]\n",
    "    )\n",
    "\n",
    "    # 2) Build a rename map by inspecting each column name\n",
    "    rename_map = {}\n",
    "    for col in df.columns:\n",
    "        c = col.lower()\n",
    "        # detect Account Name columns\n",
    "        if \"acct\" in c and \"id\" in c:\n",
    "            rename_map[col] = \"Account Name\"\n",
    "        # Contact Id\n",
    "        elif \"contact_id\" in c or \"contact id\" in c:\n",
    "            rename_map[col] = \"Contact Id\"\n",
    "        # Primary Contact variants\n",
    "        elif c.startswith(\"primary contact\"):\n",
    "            rename_map[col] = \"Primary Contact\"\n",
    "        # Active Contact\n",
    "        elif c.startswith(\"active contact\"):\n",
    "            rename_map[col] = \"Active Contact\"\n",
    "        # ConnectLink Status\n",
    "        elif \"connectlink\" in c and (\"status\".startswith(c.split()[-1]) or \"sta\" in c):\n",
    "            rename_map[col] = \"ConnectLink Status\"\n",
    "        # Agile Contact Email → Connect Link Email\n",
    "        elif \"agile contact\" in c and \"email\" in c:\n",
    "            rename_map[col] = \"Connect Link Email\"\n",
    "        # Cases / Opps\n",
    "        elif c.startswith(\"# of cas\"):\n",
    "            rename_map[col] = \"# of cases\"\n",
    "        elif c.startswith(\"# of opp\"):\n",
    "            rename_map[col] = \"# of opps\"\n",
    "        # Created Date\n",
    "        elif \"created da\" in c:\n",
    "            rename_map[col] = \"Created Date\"\n",
    "        # else—leave it untouched\n",
    "\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # 3) Verify the canonical required set is now present\n",
    "    required = [\n",
    "        \"Account Name\", \"Full Name\", \"Email\", \"Contact Id\",\n",
    "        \"Admin Role\", \"Primary Contact\", \"Active Contact\",\n",
    "        \"ConnectLink Status\", \"Connect Link Email\",\n",
    "        \"# of cases\", \"# of opps\",\n",
    "        \"Last Activity\", \"Created Date\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns after rename: {missing}\")\n",
    "\n",
    "    # 4) Normalize only the required columns\n",
    "    #   • fill blanks in text columns,\n",
    "    #   • coerce Primary Contact to boolean,\n",
    "    #   • trim & collapse whitespace on key text fields.\n",
    "    for c in required:\n",
    "        if c in df.select_dtypes(include=[\"object\",\"string\"]).columns:\n",
    "            df[c] = df[c].fillna(\"\").astype(str)\n",
    "    df[\"Primary Contact\"] = df[\"Primary Contact\"].map(\n",
    "        lambda x: str(x).strip().lower() in {\"true\",\"1\",\"yes\"}\n",
    "    )\n",
    "    for c in [\"Account Name\",\"Full Name\",\"Email\",\"Connect Link Email\"]:\n",
    "        df[c] = (\n",
    "            df[c]\n",
    "              .astype(str)\n",
    "              .str.strip()\n",
    "              .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33d4e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# STEP 2 – Hierarchy Tag (with date‐dtype coercion)\n",
    "# ----------------------------------------------------------------------\n",
    "def add_comparison_tag(df: pd.DataFrame, today: Optional[datetime] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Ensure Last Activity & Created Date are parsed as dates.\n",
    "    2) Flag privileged rows.\n",
    "    3) Build lexicographic hier_tag for sorting.\n",
    "    \"\"\"\n",
    "    if today is None:\n",
    "        today = pd.Timestamp.today().normalize()\n",
    "    \n",
    "    df = df.copy()\n",
    "\n",
    "    # — coercion: make sure these are datetime64, with errors → NaT\n",
    "    df[\"Last Activity\"]  = pd.to_datetime(df[\"Last Activity\"], errors=\"coerce\")\n",
    "    df[\"Created Date\"]   = pd.to_datetime(df[\"Created Date\"],  errors=\"coerce\")\n",
    "\n",
    "    # 1) Privileged flag\n",
    "    df[\"is_privileged\"] = (\n",
    "        df[\"Admin Role\"]\n",
    "          .astype(str)\n",
    "          .str.lower()\n",
    "          .str.strip()\n",
    "          .isin({\"owner\", \"admin\"})\n",
    "    )\n",
    "\n",
    "    # 2) Primary bit (boolean → int)\n",
    "    df[\"primary_bit\"] = df[\"Primary Contact\"].astype(bool).astype(int)\n",
    "\n",
    "    # 3) Active bit\n",
    "    df[\"active_bit\"] = (\n",
    "        df[\"Active Contact\"]\n",
    "          .astype(str)\n",
    "          .str.lower()\n",
    "          .str.strip()\n",
    "          .eq(\"active\")\n",
    "          .astype(int)\n",
    "    )\n",
    "\n",
    "    # 4) ConnectLink tier (A→3, I/U→2, blank/other→1)\n",
    "    df[\"connect_tier\"] = (\n",
    "        df[\"ConnectLink Status\"]\n",
    "          .astype(str)\n",
    "          .str.upper()\n",
    "          .str.strip()\n",
    "          .map(CONNECT_TIER)\n",
    "          .fillna(\"1\")\n",
    "    )\n",
    "\n",
    "    # 5) Opportunities bucket (Z=0, L=1–3, H=4+)\n",
    "    opps = df[\"# of opps\"].fillna(0).astype(int)\n",
    "    df[\"opps_bucket\"] = pd.cut(\n",
    "        opps,\n",
    "        bins=[-1, 0, 3, float(\"inf\")],\n",
    "        labels=[\"Z\", \"L\", \"H\"]\n",
    "    ).astype(str)\n",
    "\n",
    "    # 6) Activity tier by recency (1=≤1yr,2=1–2.5yr,3=>2.5yr,4=missing)\n",
    "    days_since = (today - df[\"Last Activity\"]).dt.days\n",
    "    df[\"activity_tier\"] = pd.cut(\n",
    "        days_since,\n",
    "        bins=[-float(\"inf\"), 365, 912, float(\"inf\")],\n",
    "        labels=[\"1\", \"2\", \"3\"]\n",
    "    ).astype(str)\n",
    "    df.loc[days_since.isna(), \"activity_tier\"] = \"4\"\n",
    "\n",
    "    # 7) U‐tier demotion for zero opps & stale\n",
    "    mask_demote = (\n",
    "        (df[\"ConnectLink Status\"].astype(str).str.upper() == \"U\") &\n",
    "        (df[\"opps_bucket\"] == \"Z\") &\n",
    "        (df[\"activity_tier\"].isin({\"3\", \"4\"}))\n",
    "    )\n",
    "    df.loc[mask_demote, \"connect_tier\"] = \"1\"\n",
    "\n",
    "    # 8) Email presence bit\n",
    "    df[\"email_bit\"] = df[\"Email\"].astype(str).str.strip().ne(\"\").astype(int)\n",
    "\n",
    "    # 9) Created date rank (older = lexicographically higher)\n",
    "    days_created = (today - df[\"Created Date\"]).dt.days.fillna(0).clip(0, 99999).astype(int)\n",
    "    df[\"created_rank\"] = days_created.astype(str).str.zfill(5)\n",
    "\n",
    "    # 10) Build the combined tag\n",
    "    df[\"hier_tag\"] = (\n",
    "        df[\"primary_bit\"].astype(str) + \"|\" +\n",
    "        df[\"active_bit\"].astype(str)  + \"|\" +\n",
    "        df[\"connect_tier\"]            + \"|\" +\n",
    "        df[\"opps_bucket\"]             + \"|\" +\n",
    "        df[\"activity_tier\"]           + \"|\" +\n",
    "        df[\"email_bit\"].astype(str)   + \"|\" +\n",
    "        df[\"created_rank\"]\n",
    "    )\n",
    "\n",
    "    # 11) Overwrite privileged rows with sentinel\n",
    "    df.loc[df[\"is_privileged\"], \"hier_tag\"] = \"PRIV\"\n",
    "\n",
    "    # 12) Drop the helper bits before returning\n",
    "    return df.drop(columns=[\n",
    "        \"primary_bit\", \"active_bit\", \"connect_tier\",\n",
    "        \"opps_bucket\", \"activity_tier\", \"email_bit\",\n",
    "        \"created_rank\"\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5130c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4 – Helpers for clustering\n",
    "def split_email(e: str) -> Tuple[str,str]:\n",
    "    \"\"\"Split an email string into (local_part, domain).\"\"\"\n",
    "    if \"@\" not in e:\n",
    "        return e, \"\"\n",
    "    return e.split(\"@\", 1)\n",
    "\n",
    "def _prep_normalised_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create helper columns for clustering:\n",
    "      • email_norm: lower/trimmed\n",
    "      • name_norm: letters+spaces only, lower\n",
    "      • sfi_key: surname + first initial\n",
    "      • name_prefix: first two chars of name_norm\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"email_norm\"]  = df[\"Email\"].str.lower().str.strip()\n",
    "    df[\"name_norm\"]   = (\n",
    "        df[\"Full Name\"]\n",
    "          .str.lower()\n",
    "          .str.replace(r\"[^a-z ]\",\"\",regex=True)\n",
    "          .str.strip()\n",
    "    )\n",
    "    df[\"sfi_key\"]     = df[\"name_norm\"].apply(lambda s: f\"{s.split()[-1]}_{s[0]}\" if s else \"\")\n",
    "    df[\"name_prefix\"] = df[\"name_norm\"].str[:2]\n",
    "    return df\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"Disjoint‐set for merging overlapping duplicate clusters.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "        self.rank   = {}\n",
    "    def find(self, x):\n",
    "        p = self.parent.get(x, x)\n",
    "        if p != x:\n",
    "            self.parent[x] = self.find(p)\n",
    "        return self.parent.get(x, x)\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb:\n",
    "            return\n",
    "        # Attach smaller‐rank tree under larger\n",
    "        if self.rank.get(ra,0) < self.rank.get(rb,0):\n",
    "            self.parent[ra] = rb\n",
    "        else:\n",
    "            self.parent[rb] = ra\n",
    "            if self.rank.get(ra,0) == self.rank.get(rb,0):\n",
    "                self.rank[ra] = self.rank.get(ra,0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f0a1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5 – STEP 4: Generate duplicate‐candidate clusters\n",
    "def add_duplicate_cluster_ids(\n",
    "    df_in: pd.DataFrame,\n",
    "    name_sim_threshold: int = 95,\n",
    "    email_edit_distance: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Within each Account Name:\n",
    "      1. Exact‐email blocking\n",
    "      2. Surname‐first‐initial blocking\n",
    "      3. Domain‐anchored fuzzy local‐part blocking\n",
    "      4. Fuzzy‐name merges on token_sort_ratio ≥ threshold\n",
    "    Assigns a stable dupe_cluster_id to each row.\n",
    "    \"\"\"\n",
    "    df = _prep_normalised_fields(df_in)\n",
    "    uf = UnionFind()\n",
    "\n",
    "    # Process each school separately\n",
    "    for acct, grp in df.groupby(\"Account Name\"):\n",
    "        idxs = grp.index.tolist()\n",
    "\n",
    "        # 1) Exact email\n",
    "        for _, block in grp.groupby(\"email_norm\"):\n",
    "            ids = block.index.tolist()\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 2) SFI blocking\n",
    "        for _, block in grp.groupby(\"sfi_key\"):\n",
    "            ids = block.index.tolist()\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 3) Domain‐anchored fuzzy email\n",
    "        for dom, sub in grp.groupby(grp[\"email_norm\"].str.split(\"@\").str[1].fillna(\"\")):\n",
    "            ids = sub.index.tolist()\n",
    "            for i,j in combinations(ids, 2):\n",
    "                li, di = split_email(df.at[i, \"email_norm\"])\n",
    "                lj, dj = split_email(df.at[j, \"email_norm\"])\n",
    "                if di == dj and distance.Levenshtein.distance(li, lj) <= email_edit_distance:\n",
    "                    uf.union(i, j)\n",
    "\n",
    "        # 4) Name‐prefix blocking + fuzzy-name\n",
    "        for _, block in grp.groupby(\"name_prefix\"):\n",
    "            ids = block.index.tolist()\n",
    "            for i,j in combinations(ids, 2):\n",
    "                if uf.find(i) == uf.find(j):\n",
    "                    continue\n",
    "                n1, n2 = df.at[i, \"name_norm\"], df.at[j, \"name_norm\"]\n",
    "                if abs(len(n1)-len(n2)) <= 2 and SequenceMatcher(None, n1, n2).ratio()*100 >= name_sim_threshold:\n",
    "                    uf.union(i, j)\n",
    "\n",
    "    # Build and attach cluster IDs\n",
    "    roots, clusters = {}, []\n",
    "    counter = 1\n",
    "    for i in df.index:\n",
    "        root = uf.find(i)\n",
    "        if root not in roots:\n",
    "            roots[root] = f\"C{counter:05d}\"\n",
    "            counter += 1\n",
    "        clusters.append(roots[root])\n",
    "\n",
    "    df_in[\"dupe_cluster_id\"] = clusters\n",
    "    return df_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "90ab2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6 – STEP 5: Choose canonical records\n",
    "def assign_canonical_records(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each duplicate cluster:\n",
    "      • Singletons → keep\n",
    "      • Privileged → keep all privileged, merge rest\n",
    "      • Else → pick winner by hier_tag, merge or needs_review\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    df[\"is_canonical\"]         = False\n",
    "    df[\"canonical_contact_id\"] = None\n",
    "    df[\"resolution_status\"]    = None\n",
    "\n",
    "    def pick_primary(rows: pd.DataFrame) -> pd.Series:\n",
    "        return rows.sort_values(\"Created Date\").iloc[0]\n",
    "\n",
    "    for cid, idxs in df.groupby(\"dupe_cluster_id\").groups.items():\n",
    "        sub = df.loc[idxs]\n",
    "\n",
    "        # singleton\n",
    "        if len(sub) == 1:\n",
    "            i = sub.index[0]\n",
    "            df.at[i, \"is_canonical\"] = True\n",
    "            df.at[i, \"canonical_contact_id\"] = df.at[i, \"Contact Id\"]\n",
    "            df.at[i, \"resolution_status\"] = \"single_record\"\n",
    "            continue\n",
    "\n",
    "        # privileged siphon\n",
    "        priv = sub[sub[\"is_privileged\"]]\n",
    "        nonp = sub[~sub[\"is_privileged\"]]\n",
    "        if not priv.empty:\n",
    "            primary = pick_primary(priv)\n",
    "            pid = primary[\"Contact Id\"]\n",
    "\n",
    "            # keep all privileged\n",
    "            df.loc[priv.index, \"is_canonical\"] = True\n",
    "            df.loc[priv.index, \"canonical_contact_id\"] = priv[\"Contact Id\"].values\n",
    "            df.loc[priv.index, \"resolution_status\"] = \"keep_privileged\"\n",
    "\n",
    "            # merge non-privileged into that primary\n",
    "            df.loc[nonp.index, \"canonical_contact_id\"] = pid\n",
    "            df.loc[nonp.index, \"resolution_status\"] = \"merge_into_privileged\"\n",
    "            continue\n",
    "\n",
    "        # no privileged: pick by hier_tag\n",
    "        sorted_sub = sub.sort_values(\"hier_tag\", ascending=False)\n",
    "        top_tag    = sorted_sub.iloc[0][\"hier_tag\"]\n",
    "        tied       = sorted_sub[sorted_sub[\"hier_tag\"] == top_tag]\n",
    "\n",
    "        # clear single winner\n",
    "        if len(tied) == 1:\n",
    "            win = tied.index[0]\n",
    "            cid_win = tied.iloc[0][\"Contact Id\"]\n",
    "\n",
    "            df.loc[win, \"is_canonical\"] = True\n",
    "            df.loc[win, \"canonical_contact_id\"] = cid_win\n",
    "            df.loc[win, \"resolution_status\"] = \"keep\"\n",
    "\n",
    "            losers = sorted_sub.index.difference([win])\n",
    "            df.loc[losers, \"canonical_contact_id\"] = cid_win\n",
    "            df.loc[losers, \"resolution_status\"] = \"merge\"\n",
    "        else:\n",
    "            # tie → needs review\n",
    "            first_cid = tied.iloc[0][\"Contact Id\"]\n",
    "\n",
    "            df.loc[tied.index, \"is_canonical\"] = True\n",
    "            df.loc[tied.index, \"canonical_contact_id\"] = tied[\"Contact Id\"].values\n",
    "            df.loc[tied.index, \"resolution_status\"] = \"needs_review\"\n",
    "\n",
    "            rest = sorted_sub.index.difference(tied.index)\n",
    "            df.loc[rest, \"canonical_contact_id\"] = first_cid\n",
    "            df.loc[rest, \"resolution_status\"] = \"merge\"\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "28d8cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7 – STEP 5a: Merge or Inactivate by email\n",
    "def levenshtein(s: str, t: str) -> int:\n",
    "    \"\"\"Classic DP implementation of edit-distance between two strings.\"\"\"\n",
    "    m, n = len(s), len(t)\n",
    "    if m < n:\n",
    "        return levenshtein(t, s)\n",
    "    if n == 0:\n",
    "        return m\n",
    "    prev = list(range(n+1))\n",
    "    for i, sc in enumerate(s, start=1):\n",
    "        curr = [i] + [0]*n\n",
    "        for j, tc in enumerate(t, start=1):\n",
    "            insert  = curr[j-1] + 1\n",
    "            delete  = prev[j] + 1\n",
    "            replace = prev[j-1] + (sc != tc)\n",
    "            curr[j] = min(insert, delete, replace)\n",
    "        prev = curr\n",
    "    return prev[n]\n",
    "\n",
    "def apply_email_merge_or_inactivate(df: pd.DataFrame,\n",
    "                                    max_email_dist: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each non-canonical row:\n",
    "      • Merge if same domain & edit-distance ≤1 (but not on initials)\n",
    "      • Else mark inactive\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"email_norm\"] = df[\"Email\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    mask_nc = ~df[\"is_canonical\"].fillna(False)\n",
    "    df.loc[mask_nc, [\"resolution_status\",\"canonical_contact_id\"]] = [None, None]\n",
    "\n",
    "    # Build lookup of canonical rows\n",
    "    can_lookup: Dict[str,List[Tuple[int,str,str,str,str]]] = {}\n",
    "    for cid, sub in df[df[\"is_canonical\"]].groupby(\"dupe_cluster_id\"):\n",
    "        can_lookup[cid] = [\n",
    "            (idx,\n",
    "             sub.at[idx,\"email_norm\"],\n",
    "             sub.at[idx,\"hier_tag\"],\n",
    "             sub.at[idx,\"Contact Id\"],\n",
    "             sub.at[idx,\"resolution_status\"])\n",
    "            for idx in sub.index\n",
    "        ]\n",
    "\n",
    "    def split_email(e: str) -> Tuple[str,str]:\n",
    "        if \"@\" not in e:\n",
    "            return e, \"\"\n",
    "        return e.split(\"@\",1)\n",
    "\n",
    "    for idx, row in df[mask_nc].iterrows():\n",
    "        my_local, my_dom = split_email(row[\"email_norm\"] or \"\")\n",
    "        best = None\n",
    "        for can_idx, can_email, can_tag, can_cid, can_stat in can_lookup.get(row[\"dupe_cluster_id\"], []):\n",
    "            loc, dom = split_email(can_email)\n",
    "            if dom != my_dom:\n",
    "                continue\n",
    "            dist = levenshtein(my_local, loc)\n",
    "            if dist > max_email_dist:\n",
    "                continue\n",
    "            if dist == 1 and len(my_local)==len(loc):\n",
    "                subs = [i for i,(a,b) in enumerate(zip(my_local,loc)) if a!=b]\n",
    "                if subs and subs[0] in (0,1):\n",
    "                    continue\n",
    "            if best is None or can_tag>best[1]:\n",
    "                best = (can_idx,can_tag,can_cid,can_stat)\n",
    "\n",
    "        if best:\n",
    "            _,_,t_cid,t_stat = best\n",
    "            df.at[idx,\"canonical_contact_id\"] = t_cid\n",
    "            df.at[idx,\"resolution_status\"] = (\n",
    "                \"merge_into_privileged\" if t_stat==\"keep_privileged\" else \"merge\"\n",
    "            )\n",
    "        else:\n",
    "            df.at[idx,\"resolution_status\"] = \"inactive\"\n",
    "            df.at[idx,\"canonical_contact_id\"] = None\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f37541d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7 – Revised STEP 5a: Merge or Inactivate by full‑email logic\n",
    "def apply_email_merge_or_inactivate(df: pd.DataFrame,\n",
    "                                    max_email_dist: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Four email buckets for each non-canonical row:\n",
    "      1) blank email anywhere → merge into the top canonical (wildcard)\n",
    "      2) exact match         → merge\n",
    "      3) one‑char off full   → merge if the single mismatch is not at idx=1 or idx=(@-1)\n",
    "      4) everything else     → inactive\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # normalize full email\n",
    "    df[\"email_norm\"] = df[\"Email\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # reset old statuses on non‑canonical rows\n",
    "    mask_nc = ~df[\"is_canonical\"].fillna(False)\n",
    "    df.loc[mask_nc, [\"resolution_status\", \"canonical_contact_id\"]] = [None, None]\n",
    "\n",
    "    # build cluster→canonical lookup: list of 5‑tuples\n",
    "    can_lookup: Dict[str, List[Tuple[int,str,str,str,str]]] = {}\n",
    "    for cid, sub in df[df[\"is_canonical\"]].groupby(\"dupe_cluster_id\"):\n",
    "        can_lookup[cid] = [\n",
    "            (idx,\n",
    "             sub.at[idx, \"email_norm\"],\n",
    "             sub.at[idx, \"hier_tag\"],\n",
    "             sub.at[idx, \"Contact Id\"],\n",
    "             sub.at[idx, \"resolution_status\"])\n",
    "            for idx in sub.index\n",
    "        ]\n",
    "\n",
    "    # classic Levenshtein implementation\n",
    "    def levenshtein(s: str, t: str) -> int:\n",
    "        m, n = len(s), len(t)\n",
    "        if m < n:\n",
    "            return levenshtein(t, s)\n",
    "        if n == 0:\n",
    "            return m\n",
    "        prev = list(range(n+1))\n",
    "        for i, sc in enumerate(s, start=1):\n",
    "            curr = [i] + [0]*n\n",
    "            for j, tc in enumerate(t, start=1):\n",
    "                ins  = curr[j-1] + 1\n",
    "                dele = prev[j]   + 1\n",
    "                rep  = prev[j-1] + (sc != tc)\n",
    "                curr[j] = min(ins, dele, rep)\n",
    "            prev = curr\n",
    "        return prev[n]\n",
    "\n",
    "    # process each non‑canonical row\n",
    "    for idx, row in df[mask_nc].iterrows():\n",
    "        me = row[\"email_norm\"] or \"\"\n",
    "        candidates = can_lookup.get(row[\"dupe_cluster_id\"], [])\n",
    "        best = None\n",
    "\n",
    "        # CASE 1: wildcard blank merges into highest‑priority canonical\n",
    "        if me == \"\":\n",
    "            best = max(candidates, key=lambda x: x[2], default=None)\n",
    "\n",
    "        else:\n",
    "            # scan through canonical candidates\n",
    "            for can_idx, ce, tag, cid_val, stat in candidates:\n",
    "                # CASE 1b: canonical is blank → wildcard match\n",
    "                if ce == \"\":\n",
    "                    best = (can_idx, ce, tag, cid_val, stat)\n",
    "                    break\n",
    "\n",
    "                # CASE 2: exact match\n",
    "                if me == ce:\n",
    "                    best = (can_idx, ce, tag, cid_val, stat)\n",
    "                    break\n",
    "\n",
    "                # CASE 3: one‑char off full-email\n",
    "                dist = levenshtein(me, ce)\n",
    "                if dist == 1:\n",
    "                    # locate mismatch position\n",
    "                    L = max(len(me), len(ce))\n",
    "                    a, b = me.ljust(L, \"\\0\"), ce.ljust(L, \"\\0\")\n",
    "                    mismatches = [i for i,(x,y) in enumerate(zip(a,b)) if x != y]\n",
    "                    atpos = me.find(\"@\")\n",
    "                    forbidden = {1, atpos-1} if atpos > 0 else {1}\n",
    "                    if mismatches and mismatches[0] not in forbidden:\n",
    "                        best = (can_idx, ce, tag, cid_val, stat)\n",
    "                        break\n",
    "\n",
    "        # CASE 4: everything else → inactive if no best match\n",
    "        if best:\n",
    "            # unpack the 5‑tuple (ignore local email & tag string)\n",
    "            _, _, _, tgt_cid, tgt_stat = best\n",
    "            df.at[idx, \"canonical_contact_id\"] = tgt_cid\n",
    "            df.at[idx, \"resolution_status\"] = (\n",
    "                \"merge_into_privileged\" if tgt_stat == \"keep_privileged\" else \"merge\"\n",
    "            )\n",
    "        else:\n",
    "            df.at[idx, \"canonical_contact_id\"] = None\n",
    "            df.at[idx, \"resolution_status\"]     = \"inactive\"\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e553fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# STEP 6 – Export Results (preserve all original columns + add dedupe columns)\n",
    "# -----------------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def export_dedupe_results(df: pd.DataFrame,\n",
    "                          out_path: str | Path = \"output/deduped.xlsx\"):\n",
    "    \"\"\"\n",
    "    Write your full DataFrame (all original columns, in original order)\n",
    "    plus the dedupe output fields appended at the end, to 'master_contacts',\n",
    "    and also produce 'change_log' and 'needs_review' sheets.\n",
    "    \"\"\"\n",
    "    path = Path(out_path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Determine final master sheet column order:\n",
    "    #    start with all original columns, then add the new ones\n",
    "    original_cols = list(df.columns)  # preserves their current order\n",
    "    dedupe_cols = [\n",
    "        \"canonical_contact_id\",\n",
    "        \"resolution_status\",\n",
    "        \"dupe_cluster_id\",\n",
    "        \"is_canonical\",\n",
    "        \"hier_tag\"\n",
    "    ]\n",
    "    # only append those not already present\n",
    "    final_master_cols = original_cols + [c for c in dedupe_cols if c not in original_cols]\n",
    "\n",
    "    # 2) Build master sheet\n",
    "    master_sheet = df[final_master_cols].sort_values(\n",
    "        by=[\"dupe_cluster_id\", \"is_canonical\"],\n",
    "        ascending=[True, False]\n",
    "    )\n",
    "\n",
    "    # 3) Change‑log – one row per merged record\n",
    "    merge_mask = df[\"resolution_status\"].isin({\"merge\", \"merge_into_privileged\"})\n",
    "    change_log = (\n",
    "        df.loc[merge_mask, [\"dupe_cluster_id\", \"Contact Id\",\n",
    "                            \"canonical_contact_id\", \"resolution_status\", \"hier_tag\"]]\n",
    "          .rename(columns={\"Contact Id\": \"old_contact_id\"})\n",
    "    )\n",
    "\n",
    "    # 4) Needs‑review – only those flagged for manual check\n",
    "    review_sheet = df[df[\"resolution_status\"] == \"needs_review\"][final_master_cols]\n",
    "\n",
    "    # 5) Write to Excel\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as xl:\n",
    "        master_sheet.to_excel(xl, sheet_name=\"master_contacts\", index=False)\n",
    "        change_log.to_excel(xl, sheet_name=\"change_log\",     index=False)\n",
    "        review_sheet.to_excel(xl, sheet_name=\"needs_review\", index=False)\n",
    "\n",
    "    logger.info(\"Wrote dedupe workbook to %s\", path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3bf97f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9 – Invoke the pipeline\n",
    "# Simply call `main()` to run all steps\n",
    "def main():\n",
    "    df = load_contacts(\"../data/Duplicate_Contact_Scrub.xlsx\")\n",
    "    df = add_comparison_tag(df)\n",
    "    df = add_duplicate_cluster_ids(df)\n",
    "    df = assign_canonical_records(df)\n",
    "    df = apply_email_merge_or_inactivate(df)\n",
    "    export_dedupe_results(df, \"output/deduped_contacts.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "79ea0f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elioa\\AppData\\Local\\Temp\\ipykernel_16420\\400361336.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Last Activity\"]  = pd.to_datetime(df[\"Last Activity\"], errors=\"coerce\")\n",
      "C:\\Users\\Elioa\\AppData\\Local\\Temp\\ipykernel_16420\\400361336.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Created Date\"]   = pd.to_datetime(df[\"Created Date\"],  errors=\"coerce\")\n",
      "INFO:__main__:Wrote dedupe workbook to C:\\Users\\Elioa\\OneDrive\\Projects\\data-dedup-pilot\\notebooks\\output\\deduped_contacts.xlsx\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
