{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d9ecb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, distance\n",
    "from openai import OpenAI\n",
    "\n",
    "# Additional imports for machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "859b7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Configuration and Setup\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Configure logging to show messages at INFO level and higher\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize OpenAI client (for potential embedding-based features)\n",
    "# TODO: Insert your OpenAI API key below\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Tunable thresholds for rule-based matching\n",
    "NAME_SIMILARITY_THRESHOLD = 95        # Minimum similarity ratio for name fuzzy matches\n",
    "EMAIL_EDIT_DISTANCE_THRESHOLD = 1     # Maximum Levenshtein distance for email matches\n",
    "\n",
    "# Mapping of ConnectLink Status codes to lexicographic tiers\n",
    "CONNECTLINK_STATUS_TO_TIER: Dict[str, str] = {\n",
    "    \"A\": \"3\",  # Active connections\n",
    "    \"I\": \"2\",  # Inactive connections\n",
    "    \"U\": \"2\",  # Unknown treated as Inactive\n",
    "    \"\":  \"1\"   # Blank or other statuses\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cb43012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Ingestion and Header Mapping\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def load_contacts(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the raw Excel file of contacts, renames incoming columns to the canonical\n",
    "    schema, synthesizes 'Full Name' if needed, parses date columns, normalizes\n",
    "    key text fields, and validates that all required columns are present.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (Path): Path to the input Excel file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame ready for deduplication steps.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist at the given path.\n",
    "        ValueError: If required columns are missing after processing.\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {file_path}\")\n",
    "    df = pd.read_excel(file_path, engine=\"openpyxl\", dtype=str)\n",
    "\n",
    "    # Rename upstream headers to our canonical column names\n",
    "    rename_map = {\n",
    "        \"Account Name: Acct_ID_18\": \"Account Name\",\n",
    "        \"Contact_id_18\":           \"Contact Id\",\n",
    "        \"Primary Contact Any\":     \"Primary Contact\",\n",
    "        \"Agile Contact Email\":     \"Connect Link Email\",\n",
    "        \"# of Cases\":              \"# of cases\",\n",
    "        \"# of Opps\":               \"# of opps\"\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Synthesize 'Full Name' if missing but First Name and Last Name exist\n",
    "    if \"Full Name\" not in df.columns:\n",
    "        if {\"First Name\", \"Last Name\"}.issubset(df.columns):\n",
    "            df[\"Full Name\"] = (\n",
    "                df[\"First Name\"].fillna(\"\").str.strip() + \" \" +\n",
    "                df[\"Last Name\"].fillna(\"\").str.strip()\n",
    "            ).str.strip()\n",
    "        else:\n",
    "            raise ValueError(\"Missing both 'Full Name' and 'First Name'+'Last Name' columns\")\n",
    "\n",
    "    # Parse date columns into datetime, coercing errors to NaT\n",
    "    for dt_col in [\"Last Activity\", \"Created Date\"]:\n",
    "        if dt_col in df.columns:\n",
    "            df[dt_col] = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "\n",
    "    # Validate presence of all required columns\n",
    "    required = [\n",
    "        \"Account Name\", \"Full Name\", \"Email\", \"Contact Id\",\n",
    "        \"Admin Role\", \"Primary Contact\", \"Active Contact\",\n",
    "        \"ConnectLink Status\", \"Connect Link Email\",\n",
    "        \"# of cases\", \"# of opps\", \"Last Activity\", \"Created Date\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns missing after ingestion: {missing}\")\n",
    "\n",
    "    # Normalize text fields: fill missing with empty, collapse whitespace, strip edges\n",
    "    for col in required:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = (\n",
    "                df[col].fillna(\"\")\n",
    "                      .astype(str)\n",
    "                      .str.strip()\n",
    "                      .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            )\n",
    "    # Convert 'Primary Contact' to boolean\n",
    "    df[\"Primary Contact\"] = df[\"Primary Contact\"].astype(str).str.lower().isin({\"true\",\"1\",\"yes\"})\n",
    "\n",
    "    logger.info(\"Loaded and normalized %d rows\", len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64539856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Hierarchy Tag Construction\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def build_hierarchy_tag(df: pd.DataFrame, reference_date: Optional[datetime] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds a lexicographic 'hier_tag' capturing:\n",
    "      1. Privilege (owner/admin)\n",
    "      2. Primary contact flag\n",
    "      3. Active contact flag\n",
    "      4. Connection tier\n",
    "      5. Opportunity bucket\n",
    "      6. Activity tier (recent vs stale/blank)\n",
    "      7. Primary email presence bit\n",
    "      8. Connect email presence bit\n",
    "      9. Creation seniority rank\n",
    "\n",
    "    The resulting tag string sorts such that the highest-priority records\n",
    "    compare greater lexicographically.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with required columns.\n",
    "        reference_date (datetime, optional): Date for recency calculations.\n",
    "                                            Defaults to today's date.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of input with new columns 'is_privileged' and 'hier_tag'.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if reference_date is None:\n",
    "        reference_date = pd.Timestamp.today().normalize()\n",
    "\n",
    "    # Ensure date columns are datetime\n",
    "    df[\"Last Activity\"] = pd.to_datetime(df[\"Last Activity\"], errors=\"coerce\")\n",
    "    df[\"Created Date\"] = pd.to_datetime(df[\"Created Date\"], errors=\"coerce\")\n",
    "\n",
    "    # 1. Privilege bit\n",
    "    df[\"is_privileged\"] = df[\"Admin Role\"].str.lower().isin({\"owner\", \"admin\"})\n",
    "\n",
    "    # 2. Primary contact bit\n",
    "    df[\"primary_bit\"] = df[\"Primary Contact\"].astype(bool).astype(int)\n",
    "\n",
    "    # 3. Active contact bit\n",
    "    df[\"active_bit\"] = df[\"Active Contact\"].str.lower().eq(\"active\").astype(int)\n",
    "\n",
    "    # 4. Connection tier\n",
    "    df[\"connect_tier\"] = (\n",
    "        df[\"ConnectLink Status\"].str.upper()\n",
    "                              .map(CONNECTLINK_STATUS_TO_TIER)\n",
    "                              .fillna(\"1\")\n",
    "    )\n",
    "\n",
    "    # 5. Opportunity bucket\n",
    "    opps = df[\"# of opps\"].fillna(\"0\").astype(int)\n",
    "    df[\"opps_bucket\"] = pd.cut(opps, [-1, 0, 3, float(\"inf\")], labels=[\"Z\", \"L\", \"H\"]).astype(str)\n",
    "\n",
    "    # 6. Activity tier: recent (<=548 days) vs stale/blank\n",
    "    days_since_act = (reference_date - df[\"Last Activity\"]).dt.days\n",
    "    df[\"activity_tier\"] = pd.Series([\"1\"] * len(df), index=df.index)\n",
    "    recent_mask = days_since_act <= 548\n",
    "    df.loc[recent_mask, \"activity_tier\"] = \"2\"\n",
    "\n",
    "    # 7. Primary email presence bit\n",
    "    df[\"primary_email_bit\"] = df[\"Email\"].astype(str).str.strip().ne(\"\").astype(int)\n",
    "\n",
    "    # 8. Connect email presence bit\n",
    "    df[\"connect_email_bit\"] = df[\"Connect Link Email\"].astype(str).str.strip().ne(\"\").astype(int)\n",
    "\n",
    "    # 9. Creation seniority rank (days since creation, zero-padded)\n",
    "    days_since_created = (\n",
    "        reference_date - df[\"Created Date\"]\n",
    "    ).dt.days.fillna(0).clip(0, 99999).astype(int)\n",
    "    df[\"created_rank\"] = days_since_created.astype(str).str.zfill(5)\n",
    "\n",
    "    # Combine into hierarchy tag\n",
    "    df[\"hier_tag\"] = (\n",
    "        df[\"is_privileged\"].astype(int).astype(str) + \"|\" +\n",
    "        df[\"primary_bit\"].astype(str)       + \"|\" +\n",
    "        df[\"active_bit\"].astype(str)        + \"|\" +\n",
    "        df[\"connect_tier\"]                  + \"|\" +\n",
    "        df[\"opps_bucket\"]                   + \"|\" +\n",
    "        df[\"activity_tier\"]                 + \"|\" +\n",
    "        df[\"primary_email_bit\"].astype(str)+ \"|\" +\n",
    "        df[\"connect_email_bit\"].astype(str)+ \"|\" +\n",
    "        df[\"created_rank\"]\n",
    "    )\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df = df.drop(columns=[\n",
    "        \"primary_bit\", \"active_bit\", \"connect_tier\",\n",
    "        \"opps_bucket\", \"activity_tier\",\n",
    "        \"primary_email_bit\", \"connect_email_bit\", \"created_rank\"\n",
    "    ])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42ce4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 3: Normalized-Fields Preparation\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def prepare_normalized_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds helper columns for blocking and fuzzy matching:\n",
    "      • email_norm: lowercased, stripped primary email\n",
    "      • connect_norm: lowercased, stripped Connect Link email\n",
    "      • name_norm: lowercased, letters-and-spaces-only full name\n",
    "      • sfi_key: blocking key of surname + '_' + first initial\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"email_norm\"]   = df[\"Email\"].astype(str).str.lower().str.strip()\n",
    "    df[\"connect_norm\"] = df[\"Connect Link Email\"].astype(str).str.lower().str.strip()\n",
    "    df[\"name_norm\"]    = (\n",
    "        df[\"Full Name\"].astype(str)\n",
    "                      .str.lower()\n",
    "                      .str.replace(r\"[^a-z ]\", \"\", regex=True)\n",
    "                      .str.strip()\n",
    "    )\n",
    "    def make_sfi(name: str) -> str:\n",
    "        parts = name.split()\n",
    "        return \"\" if len(parts) < 2 else f\"{parts[-1]}_{parts[0][0]}\"\n",
    "    df[\"sfi_key\"]      = df[\"name_norm\"].apply(make_sfi)\n",
    "    logger.info(\"Prepared normalized fields for clustering\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "849e13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 4: Duplicate‑Candidate Clustering\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Union‑Find (Disjoint Set) data structure to group record indices\n",
    "    when they satisfy blocking or fuzzy‑match rules.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.parent: Dict[int,int] = {}\n",
    "    def find(self, x: int) -> int:\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    def union(self, x: int, y: int) -> None:\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        if root_x != root_y:\n",
    "            self.parent[root_y] = root_x\n",
    "\n",
    "\n",
    "def cluster_records(\n",
    "    df: pd.DataFrame,\n",
    "    name_threshold: int = NAME_SIMILARITY_THRESHOLD,\n",
    "    email_dist: int   = EMAIL_EDIT_DISTANCE_THRESHOLD\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assigns each record a 'dupe_cluster_id' based on these rules within each\n",
    "    Account Name:\n",
    "      1) Exact match on primary email_norm (only non‑blank emails)\n",
    "      2) One‑character‑off primary email local‑part (same domain)\n",
    "      3) Exact match on name_norm\n",
    "      4) Fuzzy match on name_norm (token_sort_ratio ≥ name_threshold)\n",
    "    Ignores connect_norm clustering. Blank emails are skipped in step 1.\n",
    "    Returns a DataFrame with new column 'dupe_cluster_id'.\n",
    "    \"\"\"\n",
    "    df = prepare_normalized_fields(df)\n",
    "    uf = UnionFind()\n",
    "\n",
    "    # Cluster within each account group\n",
    "    for acct, grp in df.groupby(\"Account Name\"):\n",
    "        indices = list(grp.index)\n",
    "\n",
    "        # 1) Exact primary email (skip blanks)\n",
    "        non_blank = grp[grp[\"email_norm\"].ne(\"\")]\n",
    "        for _, block in non_blank.groupby(\"email_norm\"):\n",
    "            ids = list(block.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 2) One‑character‑off local‑part of primary email\n",
    "        valid = non_blank[non_blank[\"email_norm\"].str.contains(\"@\", na=False)]\n",
    "        parts = valid.assign(\n",
    "            domain = valid[\"email_norm\"].str.split(\"@\").str[1],\n",
    "            local  = valid[\"email_norm\"].str.split(\"@\").str[0]\n",
    "        )\n",
    "        for _, dgrp in parts.groupby(\"domain\"):\n",
    "            idxs = list(dgrp.index)\n",
    "            for i in range(len(idxs)):\n",
    "                for j in range(i+1, len(idxs)):\n",
    "                    if distance.Levenshtein.distance(\n",
    "                        dgrp.at[idxs[i], \"local\"],\n",
    "                        dgrp.at[idxs[j], \"local\"]\n",
    "                    ) <= email_dist:\n",
    "                        uf.union(idxs[i], idxs[j])\n",
    "\n",
    "        # 3) Exact full‑name match\n",
    "        for _, block in grp.groupby(\"name_norm\"):\n",
    "            ids = list(block.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 4) Fuzzy full‑name matching\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(i+1, len(indices)):\n",
    "                a, b = indices[i], indices[j]\n",
    "                if uf.find(a) != uf.find(b):\n",
    "                    if fuzz.token_sort_ratio(\n",
    "                        df.at[a, \"name_norm\"],\n",
    "                        df.at[b, \"name_norm\"]\n",
    "                    ) >= name_threshold:\n",
    "                        uf.union(a, b)\n",
    "\n",
    "    # Assign stable cluster IDs\n",
    "    root_to_cid: Dict[int,str] = {}\n",
    "    cluster_ids: List[str] = []\n",
    "    counter = 1\n",
    "    for idx in df.index:\n",
    "        root = uf.find(idx)\n",
    "        if root not in root_to_cid:\n",
    "            root_to_cid[root] = f\"C{counter:05d}\"\n",
    "            counter += 1\n",
    "        cluster_ids.append(root_to_cid[root])\n",
    "    df[\"dupe_cluster_id\"] = cluster_ids\n",
    "\n",
    "    logger.info(\"Assigned dupe_cluster_id to %d records\", len(df))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61e30c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 5: Canonical Selection\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def select_canonical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Within each dupe_cluster_id group, identify the record(s) with the highest\n",
    "    'hier_tag'. If there is exactly one such record, mark it as 'keep'. If there\n",
    "    are multiple tied at the top tag, mark them 'keep_tie'. All other records\n",
    "    get marked 'merge' and pointed at the chosen canonical Contact Id.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of input with new columns:\n",
    "          - is_canonical (bool)\n",
    "          - canonical_contact_id (str)\n",
    "          - resolution_status (str)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"is_canonical\"] = False\n",
    "    df[\"canonical_contact_id\"] = None\n",
    "    df[\"resolution_status\"] = None\n",
    "\n",
    "    def pick_top(sub: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        sorted_sub = sub.sort_values(\"hier_tag\", ascending=False)\n",
    "        top_tag = sorted_sub.iloc[0][\"hier_tag\"]\n",
    "        tied = sorted_sub[sorted_sub[\"hier_tag\"] == top_tag]\n",
    "        return tied, sorted_sub\n",
    "\n",
    "    for cluster_id, group in df.groupby(\"dupe_cluster_id\"):\n",
    "        tied, sorted_group = pick_top(group)\n",
    "        if len(tied) > 1:\n",
    "            # Multiple top-tier records: all are kept, others merged\n",
    "            for idx in tied.index:\n",
    "                df.at[idx, \"is_canonical\"] = True\n",
    "                df.at[idx, \"canonical_contact_id\"] = df.at[idx, \"Contact Id\"]\n",
    "                df.at[idx, \"resolution_status\"] = \"keep_tie\"\n",
    "            loser_idx = sorted_group.index.difference(tied.index)\n",
    "            for idx in loser_idx:\n",
    "                df.at[idx, \"canonical_contact_id\"] = tied.iloc[0][\"Contact Id\"]\n",
    "                df.at[idx, \"resolution_status\"] = \"merge\"\n",
    "        else:\n",
    "            # Single winner: keep it, merge the rest\n",
    "            winner_idx = tied.index[0]\n",
    "            df.at[winner_idx, \"is_canonical\"] = True\n",
    "            df.at[winner_idx, \"canonical_contact_id\"] = df.at[winner_idx, \"Contact Id\"]\n",
    "            df.at[winner_idx, \"resolution_status\"] = \"keep\"\n",
    "            loser_idx = sorted_group.index.difference([winner_idx])\n",
    "            for idx in loser_idx:\n",
    "                df.at[idx, \"canonical_contact_id\"] = df.at[winner_idx, \"Contact Id\"]\n",
    "                df.at[idx, \"resolution_status\"] = \"merge\"\n",
    "    logger.info(\"Selected canonical records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5570b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 6: Merge or Inactivate\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def merge_or_inactivate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each record not marked canonical, applies merge/inactivate logic uniformly,\n",
    "    with special handling when the keep record has a blank primary email,\n",
    "    and with a quick‑fail inactivation that still allows one‑char‑off email merges.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Privileged safeguard\n",
    "    priv_mask = df[\"is_privileged\"]\n",
    "    df.loc[priv_mask, \"is_canonical\"] = True\n",
    "    df.loc[priv_mask, \"canonical_contact_id\"] = df.loc[priv_mask, \"Contact Id\"]\n",
    "    df.loc[priv_mask, \"resolution_status\"] = \"keep_privileged\"\n",
    "\n",
    "    # 2) Build canonical info per cluster\n",
    "    canonals: Dict[str, List[Dict[str,str]]] = {}\n",
    "    for cid, group in df[df[\"is_canonical\"]].groupby(\"dupe_cluster_id\"):\n",
    "        canonals[cid] = [{\n",
    "            \"primary\": df.at[idx, \"email_norm\"],\n",
    "            \"connect\": df.at[idx, \"connect_norm\"],\n",
    "            \"name\":    df.at[idx, \"name_norm\"],\n",
    "            \"contact_id\": df.at[idx, \"Contact Id\"]\n",
    "        } for idx in group.index]\n",
    "\n",
    "    # 3) Identify non‑canonical, non‑privileged candidates\n",
    "    candidates: Dict[str,List[int]] = {}\n",
    "    for cid, group in df.groupby(\"dupe_cluster_id\"):\n",
    "        lst = [\n",
    "            idx for idx in group.index\n",
    "            if not df.at[idx,\"is_canonical\"] and not df.at[idx,\"is_privileged\"]\n",
    "        ]\n",
    "        if lst:\n",
    "            candidates[cid] = lst\n",
    "\n",
    "    # 4) Special‑case blank‑email keeps (unchanged)\n",
    "    for cid, cans in candidates.items():\n",
    "        keeps = canonals.get(cid, [])\n",
    "        if len(keeps)==1 and keeps[0][\"primary\"]==\"\":\n",
    "            keep_id = keeps[0][\"contact_id\"]\n",
    "            if len(cans)==1:\n",
    "                idx = cans[0]\n",
    "                df.at[idx,\"resolution_status\"] = \"merge_exact\"\n",
    "                df.at[idx,\"canonical_contact_id\"] = keep_id\n",
    "                continue\n",
    "            keep_con = keeps[0][\"connect\"]\n",
    "            # 4a) primary‐to‐connect match\n",
    "            prim = [i for i in cans if df.at[i,\"email_norm\"]==keep_con]\n",
    "            if len(prim)==1:\n",
    "                winner = prim[0]\n",
    "            elif len(prim)>1:\n",
    "                winner = max(prim, key=lambda i: df.at[i,\"hier_tag\"])\n",
    "            else:\n",
    "                # 4b) connect‐to‐connect match\n",
    "                conn = [i for i in cans if df.at[i,\"connect_norm\"]==keep_con]\n",
    "                if len(conn)==1:\n",
    "                    winner = conn[0]\n",
    "                elif len(conn)>1:\n",
    "                    winner = max(conn, key=lambda i: df.at[i,\"hier_tag\"])\n",
    "                else:\n",
    "                    # 4c) fallback\n",
    "                    winner = max(cans, key=lambda i: df.at[i,\"hier_tag\"])\n",
    "            # merge winner, inactivate others\n",
    "            for i in cans:\n",
    "                if i==winner:\n",
    "                    df.at[i,\"resolution_status\"]=\"merge_exact\"\n",
    "                    df.at[i,\"canonical_contact_id\"]=keep_id\n",
    "                else:\n",
    "                    df.at[i,\"resolution_status\"]=\"inactive\"\n",
    "                    df.at[i,\"canonical_contact_id\"]=None\n",
    "            continue\n",
    "\n",
    "    # 5) Standard logic for all other rows\n",
    "    for idx, row in df[(~df[\"is_canonical\"]) & (~df[\"is_privileged\"])].iterrows():\n",
    "        # skip already‑handled\n",
    "        if pd.notna(row[\"resolution_status\"]):\n",
    "            continue\n",
    "\n",
    "        S = canonals.get(row[\"dupe_cluster_id\"], [])\n",
    "        if not S:\n",
    "            continue\n",
    "        keep_pri = S[0][\"primary\"]\n",
    "        r_pri, r_con = row[\"email_norm\"], row[\"connect_norm\"]\n",
    "\n",
    "        # 5a) Quick‑fail inactivate if both have non‑blank emails but differ by >1 char\n",
    "            # 5a) Email‑based merge or inactive\n",
    "        if r_pri or r_con:\n",
    "            for C in S:\n",
    "                # … your merge_exact logic …\n",
    "                if not matched:\n",
    "                    df.at[idx, \"resolution_status\"] = \"inactive\"\n",
    "                continue     # ← add this here so you don't fall into name logic\n",
    "\n",
    "\n",
    "        # 5b) Email‑based merge or inactive\n",
    "        matched = False\n",
    "        if r_pri or r_con:\n",
    "            for C in S:\n",
    "                for em in (C[\"primary\"], C[\"connect\"]):\n",
    "                    if not em:\n",
    "                        continue\n",
    "                    dist = distance.Levenshtein.distance\n",
    "                    if (\n",
    "                        (r_pri and (r_pri == em or dist(r_pri, em) <= EMAIL_EDIT_DISTANCE_THRESHOLD)) or\n",
    "                        (r_con and (r_con == em or dist(r_con, em) <= EMAIL_EDIT_DISTANCE_THRESHOLD))\n",
    "                    ):\n",
    "                        df.at[idx, \"resolution_status\"] = \"merge_exact\"\n",
    "                        df.at[idx, \"canonical_contact_id\"] = C[\"contact_id\"]\n",
    "                        matched = True\n",
    "                        break\n",
    "                if matched:\n",
    "                    break\n",
    "            if not matched:\n",
    "                df.at[idx, \"resolution_status\"] = \"inactive\"\n",
    "            continue\n",
    "\n",
    "        # 5c) Name‑based merge for truly blank‑email rows\n",
    "        matched = False\n",
    "        for C in S:\n",
    "            if row[\"name_norm\"] == C[\"name\"]:\n",
    "                df.at[idx, \"resolution_status\"] = \"merge_name_exact\"\n",
    "                df.at[idx, \"canonical_contact_id\"] = C[\"contact_id\"]\n",
    "                matched = True\n",
    "                break\n",
    "        if matched:\n",
    "            continue\n",
    "        for C in S:\n",
    "            if fuzz.token_sort_ratio(row[\"name_norm\"], C[\"name\"]) >= NAME_SIMILARITY_THRESHOLD:\n",
    "                df.at[idx, \"resolution_status\"] = \"merge_name_fuzzy\"\n",
    "                df.at[idx, \"canonical_contact_id\"] = C[\"contact_id\"]\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            df.at[idx, \"resolution_status\"] = \"WIP\"\n",
    "\n",
    "    logger.info(\"Applied updated merge/inactivate logic\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf6632ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 7: Feature Engineering for Machine Learning\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def extract_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Constructs a feature matrix and label vector from the processed DataFrame:\n",
    "      - Numeric versions of hierarchy bits (privilege, primary, active)\n",
    "      - Numeric connect tier, opportunity bucket, activity days, creation days\n",
    "      - Name/email similarity to chosen canonical record\n",
    "      - Cluster size\n",
    "    Labels are taken from 'resolution_status' for supervised learning.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.Series): Encoded labels\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Ensure name_norm and email_norm exist\n",
    "    df = prepare_normalized_fields(df)\n",
    "    # Identify canonical per cluster\n",
    "    canon_map: Dict[str, Dict[str, str]] = {}\n",
    "    for cid, group in df[df[\"is_canonical\"]].groupby(\"dupe_cluster_id\"):\n",
    "        canon_map[cid] = {\n",
    "            \"email_norm\": group.iloc[0][\"email_norm\"],\n",
    "            \"name_norm\":  group.iloc[0][\"name_norm\"]\n",
    "        }\n",
    "    # Compute features\n",
    "    feature_dicts: List[Dict[str, float]] = []\n",
    "    labels: List[str] = []\n",
    "    for idx, row in df.iterrows():\n",
    "        cid = row[\"dupe_cluster_id\"]\n",
    "        canon_vals = canon_map.get(cid, {\"email_norm\":\"\",\"name_norm\":\"\"})\n",
    "        # Hierarchy bits\n",
    "        feat: Dict[str, float] = {}\n",
    "        feat[\"is_privileged\"] = float(row[\"is_privileged\"])\n",
    "        feat[\"primary_bit\"]    = float(row[\"Primary Contact\"])\n",
    "        feat[\"active_bit\"]     = float(row[\"Active Contact\"].lower()==\"active\")\n",
    "        # Connect tier numeric\n",
    "        feat[\"connect_tier\"]   = float(CONNECTLINK_STATUS_TO_TIER.get(row[\"ConnectLink Status\"].upper(), \"1\"))\n",
    "        # Opportunity bucket numeric\n",
    "        opps = int(row[\"# of opps\"] or 0)\n",
    "        feat[\"opps_bucket\"]    = float(0 if opps==0 else 1 if opps<=3 else 2)\n",
    "        # Activity recency days\n",
    "        days_act = (pd.Timestamp.today().normalize() - row[\"Last Activity\"]).days if pd.notna(row[\"Last Activity\"]) else 0\n",
    "        feat[\"days_since_activity\"] = float(days_act)\n",
    "        # Creation seniority days\n",
    "        days_cr = (pd.Timestamp.today().normalize() - row[\"Created Date\"]).days if pd.notna(row[\"Created Date\"]) else 0\n",
    "        feat[\"days_since_created\"] = float(days_cr)\n",
    "        # Similarities to canonical\n",
    "        feat[\"name_similarity\"] = float(fuzz.token_sort_ratio(row[\"name_norm\"], canon_vals[\"name_norm\"]))\n",
    "        feat[\"email_edit_dist\"] = float(distance.Levenshtein.distance(row[\"email_norm\"], canon_vals[\"email_norm\"]))\n",
    "        # Cluster size\n",
    "        feat[\"cluster_size\"]    = float(len(df[df[\"dupe_cluster_id\"]==cid]))\n",
    "        feature_dicts.append(feat)\n",
    "        labels.append(row[\"resolution_status\"])\n",
    "    X = pd.DataFrame(feature_dicts)\n",
    "    # Encode string labels to integers\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels)\n",
    "    # Persist label encoder for inference\n",
    "    df_label_enc = pd.DataFrame({\"label\": labels})\n",
    "    return X, pd.Series(y), encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e5ac2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 8: Model Training, Evaluation, Saving\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def train_and_save_model(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    encoder: LabelEncoder,\n",
    "    model_path: Path,\n",
    "    encoder_path: Path\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Splits features and labels into train/test, trains a RandomForest classifier,\n",
    "    evaluates performance, and saves both the trained model and label encoder.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    logger.info(\"Model evaluation:\\n%s\", report)\n",
    "\n",
    "    # Save model and encoder\n",
    "    joblib.dump(clf, model_path)\n",
    "    joblib.dump(encoder, encoder_path)\n",
    "    logger.info(\"Saved trained model to %s and encoder to %s\", model_path, encoder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eab85030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 9: Model Loading and Inference\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def load_and_apply_model(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: Path,\n",
    "    encoder_path: Path\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a saved classifier and label encoder, computes features for the DataFrame,\n",
    "    predicts resolution_status for each record, and appends a new column\n",
    "    'ml_resolution_status' with the decoded predictions.\n",
    "    \"\"\"\n",
    "    # Load model and encoder\n",
    "    clf: RandomForestClassifier = joblib.load(model_path)\n",
    "    encoder: LabelEncoder = joblib.load(encoder_path)\n",
    "\n",
    "    # Extract features (we discard the returned encoder)\n",
    "    X, _, _ = extract_features(df)\n",
    "    y_pred = clf.predict(X)\n",
    "    decoded = encoder.inverse_transform(y_pred)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"ml_resolution_status\"] = decoded\n",
    "    logger.info(\"Applied ML model to %d records\", len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a676bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Full Pipeline Invocation\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def run_pipeline(\n",
    "    input_path: Path,\n",
    "    output_path: Path,\n",
    "    model_path: Optional[Path]   = None,\n",
    "    encoder_path: Optional[Path] = None,\n",
    "    train_model_flag: bool       = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executes the full pipeline:\n",
    "      1) Ingest contacts\n",
    "      2) Build hierarchy tags\n",
    "      3) Cluster duplicates\n",
    "      4) Select canonical records\n",
    "      5) Merge or inactivate others\n",
    "      6) Optionally train and save an ML model on this output\n",
    "      7) Optionally load and apply an existing ML model\n",
    "\n",
    "    Parameters:\n",
    "        input_path (Path): Path to raw Excel input.\n",
    "        output_path (Path): Path where results (and Excel export) will be written.\n",
    "        model_path (Path, optional): Where to save or load the trained classifier.\n",
    "        encoder_path (Path, optional): Where to save or load the label encoder.\n",
    "        train_model_flag (bool): If True, trains a model on current data and saves it.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Final DataFrame with both rule-based and ML-based resolution statuses.\n",
    "    \"\"\"\n",
    "    df = load_contacts(input_path)\n",
    "    df = build_hierarchy_tag(df)\n",
    "    df = cluster_records(df)\n",
    "    df = select_canonical(df)\n",
    "    df = merge_or_inactivate(df)\n",
    "\n",
    "    # Optional training\n",
    "    if train_model_flag and model_path and encoder_path:\n",
    "        X, y, encoder = extract_features(df)\n",
    "        train_and_save_model(X, y, encoder, model_path, encoder_path)\n",
    "    # Optional inference\n",
    "    if model_path and encoder_path and not train_model_flag:\n",
    "        if model_path.exists() and encoder_path.exists():\n",
    "            df = load_and_apply_model(df, model_path, encoder_path)\n",
    "        else:\n",
    "            logger.warning(\n",
    "            \"Model or encoder file not found at %s or %s; skipping ML inference\",\n",
    "            model_path, encoder_path\n",
    "        )\n",
    "\n",
    "  # Export logic: write full results to Excel for comparison\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "        # Full dataset including rule-based and ML-based statuses\n",
    "        df.to_excel(writer, sheet_name=\"results\", index=False)\n",
    "    logger.info(\"Exported results to %s\", output_path)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "deb1e3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded and normalized 46523 rows\n",
      "INFO:__main__:Prepared normalized fields for clustering\n",
      "INFO:__main__:Assigned dupe_cluster_id to 46523 records\n",
      "INFO:__main__:Selected canonical records\n",
      "INFO:__main__:Applied updated merge/inactivate logic\n",
      "WARNING:__main__:Model or encoder file not found at models\\rf_model.joblib or models\\label_encoder.joblib; skipping ML inference\n",
      "INFO:__main__:Exported results to output\\dedup_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "df = run_pipeline(\n",
    "    input_path=Path(\"../data/Duplicate Contact Scrub.xlsx\"),\n",
    "    output_path=Path(\"output/dedup_results.xlsx\"),\n",
    "    model_path=Path(\"models/rf_model.joblib\"),\n",
    "    encoder_path=Path(\"models/label_encoder.joblib\"),\n",
    "    train_model_flag=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
