{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3893250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9ad650ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import combinations\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Only-used constants\n",
    "CONNECT_TIER = {\"A\": \"3\", \"I\": \"2\", \"U\": \"2\", \"\": \"1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c19e15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 1 – Ingestion & Validation\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_contacts(file_path: str | Path) -> pd.DataFrame:\n",
    "    path = Path(file_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    required = [\n",
    "        \"Account Name\", \"Full Name\", \"Email\", \"Contact Id\",\n",
    "        \"Admin Role\", \"Primary Contact\", \"Active Contact\",\n",
    "        \"ConnectLink Status\", \"Connect Link Email\",\n",
    "        \"# of cases\", \"# of opps\", \"Last Activity\", \"Created Date\"\n",
    "    ]\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\", parse_dates=[\"Last Activity\",\"Created Date\"])\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "    df = df[required]\n",
    "    # fill blanks\n",
    "    df = df.fillna({c: \"\" for c in df.select_dtypes(include=[\"object\",\"string\"]).columns})\n",
    "    df[\"Primary Contact\"] = df[\"Primary Contact\"].fillna(False)\n",
    "    # trim whitespace\n",
    "    for c in [\"Account Name\",\"Full Name\",\"Email\",\"Connect Link Email\"]:\n",
    "        df[c] = df[c].astype(str).str.strip().str.replace(r\"\\s+\",\" \",regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1f1d2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 2 – Hierarchy Tag\n",
    "# -----------------------------------------------------------------------------\n",
    "def add_comparison_tag(df: pd.DataFrame, today: Optional[datetime]=None) -> pd.DataFrame:\n",
    "    if today is None:\n",
    "        today = pd.Timestamp.today().normalize()\n",
    "    df = df.copy()\n",
    "    df[\"is_privileged\"] = df[\"Admin Role\"].str.lower().str.strip().isin({\"owner\",\"admin\"})\n",
    "    df[\"primary_bit\"]   = df[\"Primary Contact\"].astype(bool).astype(int)\n",
    "    df[\"active_bit\"]    = (df[\"Active Contact\"].str.lower().str.strip()==\"active\").astype(int)\n",
    "    df[\"connect_tier\"]  = df[\"ConnectLink Status\"].str.upper().str.strip().map(CONNECT_TIER).fillna(\"1\")\n",
    "    opps = df[\"# of opps\"].fillna(0).astype(int)\n",
    "    df[\"opps_bucket\"]   = pd.cut(opps,[-1,0,3,float(\"inf\")],labels=[\"Z\",\"L\",\"H\"]).astype(str)\n",
    "    days = (today - df[\"Last Activity\"]).dt.days\n",
    "    df[\"activity_tier\"] = pd.cut(days,[-float(\"inf\"),365,912,float(\"inf\")],labels=[\"1\",\"2\",\"3\"]).astype(str)\n",
    "    df.loc[days.isna(),\"activity_tier\"]=\"4\"\n",
    "    demote = (df[\"ConnectLink Status\"]==\"U\") & (df[\"opps_bucket\"]==\"Z\") & (df[\"activity_tier\"].isin({\"3\",\"4\"}))\n",
    "    df.loc[demote,\"connect_tier\"]=\"1\"\n",
    "    df[\"email_bit\"]     = df[\"Email\"].astype(str).str.strip().ne(\"\").astype(int)\n",
    "    crdays = (today - df[\"Created Date\"]).dt.days.fillna(0).clip(0,99999).astype(int)\n",
    "    df[\"created_rank\"]  = crdays.astype(str).str.zfill(5)\n",
    "    df[\"hier_tag\"]      = (\n",
    "        df[\"primary_bit\"].astype(str)+\"|\"+\n",
    "        df[\"active_bit\"].astype(str)+\"|\"+\n",
    "        df[\"connect_tier\"]+\"|\"+\n",
    "        df[\"opps_bucket\"]+\"|\"+\n",
    "        df[\"activity_tier\"]+\"|\"+\n",
    "        df[\"email_bit\"].astype(str)+\"|\"+\n",
    "        df[\"created_rank\"]\n",
    "    )\n",
    "    df.loc[df[\"is_privileged\"],\"hier_tag\"]=\"PRIV\"\n",
    "    return df.drop(columns=[\"primary_bit\",\"active_bit\",\"connect_tier\",\"opps_bucket\",\"activity_tier\",\"email_bit\",\"created_rank\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c5e6d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Helpers for clustering\n",
    "# -----------------------------------------------------------------------------\n",
    "def split_email(e: str) -> Tuple[str,str]:\n",
    "    if \"@ \" not in e and \"@\" not in e:\n",
    "        return e, \"\"\n",
    "    return e.split(\"@\",1)  # local,domain\n",
    "\n",
    "def _prep_normalised_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"email_norm\"] = df[\"Email\"].str.lower().str.strip()\n",
    "    df[\"name_norm\"]  = (\n",
    "        df[\"Full Name\"].str.lower().str.replace(r\"[^a-z ]\",\"\",regex=True).str.strip()\n",
    "    )\n",
    "    df[\"sfi_key\"]    = df[\"name_norm\"].apply(lambda s: f\"{s.split()[-1]}_{s[0]}\" if s else \"\")\n",
    "    df[\"name_prefix\"]= df[\"name_norm\"].str[:2]\n",
    "    return df\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent,self.rank={},{}\n",
    "    def find(self,x):\n",
    "        p=self.parent.get(x,x)\n",
    "        if p!=x: self.parent[x]=self.find(p)\n",
    "        return self.parent.get(x,x)\n",
    "    def union(self,a,b):\n",
    "        ra,rb=self.find(a),self.find(b)\n",
    "        if ra==rb: return\n",
    "        if self.rank.get(ra,0)<self.rank.get(rb,0):\n",
    "            self.parent[ra]=rb\n",
    "        else:\n",
    "            self.parent[rb]=ra\n",
    "            if self.rank.get(ra,0)==self.rank.get(rb,0):\n",
    "                self.rank[ra]=self.rank.get(ra,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "046998ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# STEP 4 – Duplicate-Candidate Generation\n",
    "# -----------------------------------------------------------------------------\n",
    "def add_duplicate_cluster_ids(\n",
    "    df_in: pd.DataFrame,\n",
    "    name_sim_threshold: int = 95,\n",
    "    email_edit_distance: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    df = _prep_normalised_fields(df_in)\n",
    "    uf = UnionFind()\n",
    "\n",
    "    for acct, grp in df.groupby(\"Account Name\"):\n",
    "        ids = grp.index.tolist()\n",
    "        # exact email\n",
    "        for _,blk in grp.groupby(\"email_norm\"):\n",
    "            idxs=blk.index.tolist()\n",
    "            for i in idxs[1:]: uf.union(idxs[0],i)\n",
    "        # surname+initial\n",
    "        for _,blk in grp.groupby(\"sfi_key\"):\n",
    "            idxs=blk.index.tolist()\n",
    "            for i in idxs[1:]: uf.union(idxs[0],i)\n",
    "        # domain‑anchored fuzzy email\n",
    "        for dom,sub in grp.groupby(grp[\"email_norm\"].str.split(\"@\").str[1].fillna(\"\")):\n",
    "            idxs=sub.index.tolist()\n",
    "            for i,j in combinations(idxs,2):\n",
    "                li,di=split_email(df.at[i,\"email_norm\"])\n",
    "                lj,dj=split_email(df.at[j,\"email_norm\"])\n",
    "                if di==dj and distance.Levenshtein.distance(li,lj)<=email_edit_distance:\n",
    "                    uf.union(i,j)\n",
    "        # name_prefix blocking for fuzzy-name\n",
    "        for _,blk in grp.groupby(\"name_prefix\"):\n",
    "            idxs=blk.index.tolist()\n",
    "            for i,j in combinations(idxs,2):\n",
    "                if uf.find(i)==uf.find(j): continue\n",
    "                n1,n2=df.at[i,\"name_norm\"],df.at[j,\"name_norm\"]\n",
    "                if abs(len(n1)-len(n2))<=2 and fuzz.token_sort_ratio(n1,n2)>=name_sim_threshold:\n",
    "                    uf.union(i,j)\n",
    "\n",
    "    # build cluster ids on same DataFrame\n",
    "    roots,clusters={},[]\n",
    "    cnt=1\n",
    "    for i in df.index:\n",
    "        r=uf.find(i)\n",
    "        if r not in roots:\n",
    "            roots[r]=f\"C{cnt:05d}\"; cnt+=1\n",
    "        clusters.append(roots[r])\n",
    "    df_in[\"dupe_cluster_id\"]=clusters\n",
    "    return df_in\n",
    "\n",
    "\n",
    "# %%\n",
    "def assign_canonical_records(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a copy of df_in with canonical/merge annotations.\n",
    "    Fixed: initialize columns before any reference to them.\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    \n",
    "    # 1) Initialize all three columns up front\n",
    "    df[\"is_canonical\"]         = False\n",
    "    df[\"canonical_contact_id\"] = None\n",
    "    df[\"resolution_status\"]    = None\n",
    "    \n",
    "    # 2) Helper to pick the earliest Created Date\n",
    "    def _pick_primary(rows: pd.DataFrame) -> pd.Series:\n",
    "        return rows.sort_values(\"Created Date\").iloc[0]\n",
    "    \n",
    "    # 3) Cluster‐by‐cluster logic\n",
    "    for cid, idxs in df.groupby(\"dupe_cluster_id\").groups.items():\n",
    "        sub = df.loc[idxs]\n",
    "        # 3a) Single row → keep as-is\n",
    "        if len(sub) == 1:\n",
    "            i = sub.index[0]\n",
    "            df.at[i, \"is_canonical\"]         = True\n",
    "            df.at[i, \"canonical_contact_id\"] = df.at[i, \"Contact Id\"]\n",
    "            df.at[i, \"resolution_status\"]    = \"single_record\"\n",
    "            continue\n",
    "        \n",
    "        # 3b) Privileged siphon\n",
    "        priv = sub[sub[\"is_privileged\"]]\n",
    "        nonp = sub[~sub[\"is_privileged\"]]\n",
    "        if not priv.empty:\n",
    "            # pick primary privileged by Created Date\n",
    "            primary = _pick_primary(priv)\n",
    "            pid = primary[\"Contact Id\"]\n",
    "            \n",
    "            # mark all privileged as kept\n",
    "            df.loc[priv.index, \"is_canonical\"]         = True\n",
    "            df.loc[priv.index, \"canonical_contact_id\"] = priv[\"Contact Id\"]\n",
    "            df.loc[priv.index, \"resolution_status\"]    = \"keep_privileged\"\n",
    "            \n",
    "            # merge everyone else into that primary\n",
    "            df.loc[nonp.index, \"canonical_contact_id\"] = pid\n",
    "            df.loc[nonp.index, \"resolution_status\"]    = \"merge_into_privileged\"\n",
    "            continue\n",
    "        \n",
    "        # 3c) No privileged: hierarchy‐tag winner\n",
    "        sorted_sub = sub.sort_values(\"hier_tag\", ascending=False)\n",
    "        top_tag    = sorted_sub.iloc[0][\"hier_tag\"]\n",
    "        tied       = sorted_sub[sorted_sub[\"hier_tag\"] == top_tag]\n",
    "        \n",
    "        if len(tied) == 1:\n",
    "            # clear winner\n",
    "            win = tied.index[0]\n",
    "            cid_win = tied.iloc[0][\"Contact Id\"]\n",
    "            \n",
    "            df.at[win, \"is_canonical\"]         = True\n",
    "            df.at[win, \"canonical_contact_id\"] = cid_win\n",
    "            df.at[win, \"resolution_status\"]    = \"keep\"\n",
    "            \n",
    "            losers = sorted_sub.index.difference([win])\n",
    "            df.loc[losers, \"canonical_contact_id\"] = cid_win\n",
    "            df.loc[losers, \"resolution_status\"]    = \"merge\"\n",
    "        else:\n",
    "            # tie → needs manual review\n",
    "            first_cid = tied.iloc[0][\"Contact Id\"]\n",
    "            \n",
    "            df.loc[tied.index, \"is_canonical\"]         = True\n",
    "            df.loc[tied.index, \"canonical_contact_id\"] = tied[\"Contact Id\"]\n",
    "            df.loc[tied.index, \"resolution_status\"]    = \"needs_review\"\n",
    "            \n",
    "            rest = sorted_sub.index.difference(tied.index)\n",
    "            df.loc[rest, \"canonical_contact_id\"] = first_cid\n",
    "            df.loc[rest, \"resolution_status\"]    = \"merge\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dd497709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ------------------------------------------------------------------------\n",
    "# Helper: Levenshtein edit distance (classic DP implementation)\n",
    "# ------------------------------------------------------------------------\n",
    "def levenshtein(s: str, t: str) -> int:\n",
    "    m, n = len(s), len(t)\n",
    "    if m < n:\n",
    "        return levenshtein(t, s)\n",
    "    if n == 0:\n",
    "        return m\n",
    "    prev = list(range(n + 1))\n",
    "    for i, sc in enumerate(s, start=1):\n",
    "        curr = [i] + [0]*n\n",
    "        for j, tc in enumerate(t, start=1):\n",
    "            insert = curr[j-1] + 1\n",
    "            delete = prev[j] + 1\n",
    "            replace = prev[j-1] + (sc != tc)\n",
    "            curr[j] = min(insert, delete, replace)\n",
    "        prev = curr\n",
    "    return prev[n]\n",
    "\n",
    "\n",
    "# %%\n",
    "def apply_email_merge_or_inactivate(df: pd.DataFrame,\n",
    "                                    max_email_dist: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge non-canonical rows if:\n",
    "      • Same domain\n",
    "      • Local-part edit distance ≤1\n",
    "      • And if distance ==1, the changed character is NOT at pos 0 or 1.\n",
    "    Otherwise mark as inactive.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Normalize email if not already present\n",
    "    df[\"email_norm\"] = df[\"Email\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # Reset prior statuses for non-canonical rows\n",
    "    mask_nc = ~df[\"is_canonical\"].fillna(False)\n",
    "    df.loc[mask_nc, [\"resolution_status\", \"canonical_contact_id\"]] = [None, None]\n",
    "\n",
    "    # Build lookup of canonical records by cluster\n",
    "    can_lookup: Dict[str, List[Tuple[int,str,int,str,str]]] = {}\n",
    "    for cid, sub in df[df[\"is_canonical\"]].groupby(\"dupe_cluster_id\"):\n",
    "        can_lookup[cid] = [\n",
    "            (idx,\n",
    "             sub.at[idx, \"email_norm\"],\n",
    "             sub.at[idx, \"hier_tag\"],\n",
    "             sub.at[idx, \"Contact Id\"],\n",
    "             sub.at[idx, \"resolution_status\"])\n",
    "            for idx in sub.index\n",
    "        ]\n",
    "\n",
    "    # Helper to split into local, domain\n",
    "    def split_email(e: str) -> Tuple[str,str]:\n",
    "        if \"@\" not in e:\n",
    "            return e, \"\"\n",
    "        return e.split(\"@\", 1)\n",
    "\n",
    "    # Process each non-canonical row\n",
    "    for idx, row in df[mask_nc].iterrows():\n",
    "        my_local, my_dom = split_email(row[\"email_norm\"] or \"\")\n",
    "        best_match = None  # to hold (can_idx, can_tag, can_cid, can_stat)\n",
    "\n",
    "        # Examine each canonical in same cluster\n",
    "        for can_idx, can_email, can_tag, can_cid, can_stat in can_lookup.get(row[\"dupe_cluster_id\"], []):\n",
    "            can_local, can_dom = split_email(can_email)\n",
    "            # Only consider merge candidates with exact domain match\n",
    "            if my_dom != can_dom:\n",
    "                continue\n",
    "\n",
    "            # Compute edit distance\n",
    "            dist = levenshtein(my_local, can_local)\n",
    "            if dist > max_email_dist:\n",
    "                continue  # too many errors → skip\n",
    "\n",
    "            # If exactly one edit, ensure it's not at initial positions\n",
    "            if dist == 1 and len(my_local) == len(can_local):\n",
    "                # find substitution index\n",
    "                subs = [i for i, (a, b) in enumerate(zip(my_local, can_local)) if a != b]\n",
    "                if subs and subs[0] in (0, 1):\n",
    "                    continue  # error at pos 0/1 → treat as inactive (skip)\n",
    "\n",
    "            # Passed all checks → potential merge target\n",
    "            if best_match is None or can_tag > best_match[1]:\n",
    "                best_match = (can_idx, can_tag, can_cid, can_stat)\n",
    "\n",
    "        # Apply merge or inactive\n",
    "        if best_match:\n",
    "            _, _, target_cid, target_stat = best_match\n",
    "            df.at[idx, \"canonical_contact_id\"] = target_cid\n",
    "            df.at[idx, \"resolution_status\"] = (\n",
    "                \"merge_into_privileged\"\n",
    "                if target_stat == \"keep_privileged\" else\n",
    "                \"merge\"\n",
    "            )\n",
    "        else:\n",
    "            df.at[idx, \"resolution_status\"] = \"inactive\"\n",
    "            df.at[idx, \"canonical_contact_id\"] = None\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0728282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 6 – Export Results\n",
    "# -----------------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def export_dedupe_results(df: pd.DataFrame, out_path: str | Path = \"output/deduped.xlsx\"):\n",
    "    path = Path(out_path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Master sheet – ready for CRM import\n",
    "    master_cols = [\n",
    "        \"Account Name\", \"Full Name\", \"Email\", \"Contact Id\",\n",
    "        \"canonical_contact_id\", \"resolution_status\",\n",
    "        \"dupe_cluster_id\", \"is_canonical\", \"hier_tag\"\n",
    "    ]\n",
    "    master = df[master_cols].sort_values(\n",
    "        by=[\"dupe_cluster_id\", \"is_canonical\"],\n",
    "        ascending=[True, False]\n",
    "    )\n",
    "\n",
    "    # 2) Change‑log – one row per merged record\n",
    "    merge_mask = df[\"resolution_status\"].isin({\"merge\", \"merge_into_privileged\"})\n",
    "    change = (\n",
    "        df.loc[merge_mask, [\"dupe_cluster_id\", \"Contact Id\",\n",
    "                            \"canonical_contact_id\", \"resolution_status\", \"hier_tag\"]]\n",
    "          .rename(columns={\"Contact Id\": \"old_contact_id\"})\n",
    "    )\n",
    "\n",
    "    # 3) Needs‑review sheet\n",
    "    review = df[df[\"resolution_status\"] == \"needs_review\"][master_cols]\n",
    "\n",
    "    # 4) Write to Excel\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as xl:\n",
    "        master.to_excel(xl, sheet_name=\"master_contacts\", index=False)\n",
    "        change.to_excel(xl, sheet_name=\"change_log\",     index=False)\n",
    "        review.to_excel(xl, sheet_name=\"needs_review\",   index=False)\n",
    "\n",
    "    logger.info(\"Wrote dedupe workbook to %s\", path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "50b2c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from openai import OpenAI, OpenAIError\n",
    "import logging\n",
    "\n",
    "client = OpenAI()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def review_contacts_with_llm(df: pd.DataFrame,\n",
    "                             sample_size: int = 10,\n",
    "                             name_sim_th: int = 95,\n",
    "                             email_dist_th: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Ask the LLM to scan a few sample rows plus your rules for edge cases.\n",
    "    Falls back to an empty string if we’re rate‑limited or out of quota.\n",
    "    \"\"\"\n",
    "    # … build samples, system, user as before …\n",
    "    samples = df.sample(min(sample_size, len(df))).to_dict(\"records\")\n",
    "    examples_text = \"\\n\".join(\n",
    "        f\"- Account: {r['Account Name']}, Name: {r['Full Name']}, Email: {r['Email']}, \"\n",
    "        f\"Cluster: {r.get('dupe_cluster_id','?')}, Status: {r.get('resolution_status','?')}\"\n",
    "        for r in samples\n",
    "    )\n",
    "    system = (\n",
    "        \"You are a data engineer reviewing a Python deduplication pipeline. \"\n",
    "        f\"Rules: name-sim ≥ {name_sim_th}, email-dist ≤ {email_dist_th}. \"\n",
    "        \"Identify potential misclassifications or edge cases.\"\n",
    "    )\n",
    "    user = f\"Here are sample records:\\n{examples_text}\\n\\nWhat issues do you see?\"\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\",   \"content\": user}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    except OpenAIError as e:\n",
    "        # catch RateLimitError, InsufficientQuota, etc.\n",
    "        logger.warning(\"LLM review skipped: %s\", str(e))\n",
    "        return \"\"  # or return some default note\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "79872cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.454922 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.783267 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:__main__:LLM review skipped: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "INFO:__main__:Wrote dedupe workbook to C:\\Users\\Elioa\\OneDrive\\Projects\\data-dedup-pilot\\notebooks\\output\\deduped_contacts.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM REVIEW ===\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ── Cell 2: main() definition ──\n",
    "def main():\n",
    "    df = load_contacts(\"../data/duplicate_contacts_small.xlsx\")\n",
    "    df = add_comparison_tag(df)\n",
    "    df = add_duplicate_cluster_ids(df)\n",
    "    df = assign_canonical_records(df)\n",
    "    df = apply_email_merge_or_inactivate(df)\n",
    "\n",
    "    # LLM review\n",
    "    feedback = review_contacts_with_llm(df, sample_size=8)\n",
    "    print(\"\\n=== LLM REVIEW ===\\n\", feedback)\n",
    "\n",
    "    export_dedupe_results(df, \"output/deduped_contacts.xlsx\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# %%\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
