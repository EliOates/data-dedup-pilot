{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8faed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, distance\n",
    "from openai import OpenAI\n",
    "\n",
    "# Additional imports for machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f58148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Configuration and Setup\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Configure logging to show messages at INFO level and higher\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize OpenAI client (for potential embedding-based features)\n",
    "# TODO: Insert your OpenAI API key below\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Tunable thresholds for rule-based matching\n",
    "NAME_SIMILARITY_THRESHOLD = 95        # Minimum similarity ratio for name fuzzy matches\n",
    "EMAIL_EDIT_DISTANCE_THRESHOLD = 1     # Maximum Levenshtein distance for email matches\n",
    "\n",
    "# Mapping of ConnectLink Status codes to lexicographic tiers\n",
    "CONNECTLINK_STATUS_TO_TIER: Dict[str, str] = {\n",
    "    \"A\": \"3\",  # Active connections\n",
    "    \"I\": \"2\",  # Inactive connections\n",
    "    \"U\": \"2\",  # Unknown treated as Inactive\n",
    "    \"\":  \"1\"   # Blank or other statuses\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a40f1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Ingestion and Header Mapping\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def load_contacts(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the raw Excel file of contacts, renames incoming columns to the canonical\n",
    "    schema, synthesizes 'Full Name' if needed, parses date columns, normalizes\n",
    "    key text fields, and validates that all required columns are present.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (Path): Path to the input Excel file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame ready for deduplication steps.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist at the given path.\n",
    "        ValueError: If required columns are missing after processing.\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {file_path}\")\n",
    "    df = pd.read_excel(file_path, engine=\"openpyxl\", dtype=str)\n",
    "\n",
    "    # Rename upstream headers to our canonical column names\n",
    "    rename_map = {\n",
    "        \"Account Name: Acct_ID_18\": \"Account Name\",\n",
    "        \"Contact_id_18\":           \"Contact Id\",\n",
    "        \"Primary Contact Any\":     \"Primary Contact\",\n",
    "        \"Agile Contact Email\":     \"Connect Link Email\",\n",
    "        \"# of Cases\":              \"# of cases\",\n",
    "        \"# of Opps\":               \"# of opps\"\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Synthesize 'Full Name' if missing but First Name and Last Name exist\n",
    "    if \"Full Name\" not in df.columns:\n",
    "        if {\"First Name\", \"Last Name\"}.issubset(df.columns):\n",
    "            df[\"Full Name\"] = (\n",
    "                df[\"First Name\"].fillna(\"\").str.strip() + \" \" +\n",
    "                df[\"Last Name\"].fillna(\"\").str.strip()\n",
    "            ).str.strip()\n",
    "        else:\n",
    "            raise ValueError(\"Missing both 'Full Name' and 'First Name'+'Last Name' columns\")\n",
    "\n",
    "    # Parse date columns into datetime, coercing errors to NaT\n",
    "    for dt_col in [\"Last Activity\", \"Created Date\"]:\n",
    "        if dt_col in df.columns:\n",
    "            df[dt_col] = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "\n",
    "    # Validate presence of all required columns\n",
    "    required = [\n",
    "        \"Account Name\", \"Full Name\", \"Email\", \"Contact Id\",\n",
    "        \"Admin Role\", \"Primary Contact\", \"Active Contact\",\n",
    "        \"ConnectLink Status\", \"Connect Link Email\",\n",
    "        \"# of cases\", \"# of opps\", \"Last Activity\", \"Created Date\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns missing after ingestion: {missing}\")\n",
    "\n",
    "    # Normalize text fields: fill missing with empty, collapse whitespace, strip edges\n",
    "    for col in required:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = (\n",
    "                df[col].fillna(\"\")\n",
    "                      .astype(str)\n",
    "                      .str.strip()\n",
    "                      .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            )\n",
    "    # Convert 'Primary Contact' to boolean\n",
    "    df[\"Primary Contact\"] = df[\"Primary Contact\"].astype(str).str.lower().isin({\"true\",\"1\",\"yes\"})\n",
    "\n",
    "    logger.info(\"Loaded and normalized %d rows\", len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74575256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Hierarchy Tag Construction\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def build_hierarchy_tag(df: pd.DataFrame, reference_date: Optional[datetime] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds a lexicographic 'hier_tag' capturing:\n",
    "      1. Privilege (owner/admin)\n",
    "      2. Primary contact flag\n",
    "      3. Active contact flag\n",
    "      4. Connection tier\n",
    "      5. Opportunity bucket\n",
    "      6. Activity tier (recent vs stale/blank)\n",
    "      7. Primary email presence bit\n",
    "      8. Connect email presence bit\n",
    "      9. Creation seniority rank\n",
    "\n",
    "    The resulting tag string sorts such that the highest-priority records\n",
    "    compare greater lexicographically.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with required columns.\n",
    "        reference_date (datetime, optional): Date for recency calculations.\n",
    "                                            Defaults to today's date.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of input with new columns 'is_privileged' and 'hier_tag'.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if reference_date is None:\n",
    "        reference_date = pd.Timestamp.today().normalize()\n",
    "\n",
    "    # Ensure date columns are datetime\n",
    "    df[\"Last Activity\"] = pd.to_datetime(df[\"Last Activity\"], errors=\"coerce\")\n",
    "    df[\"Created Date\"] = pd.to_datetime(df[\"Created Date\"], errors=\"coerce\")\n",
    "\n",
    "    # 1. Privilege bit\n",
    "    df[\"is_privileged\"] = df[\"Admin Role\"].str.lower().isin({\"owner\", \"admin\"})\n",
    "\n",
    "    # 2. Primary contact bit\n",
    "    df[\"primary_bit\"] = df[\"Primary Contact\"].astype(bool).astype(int)\n",
    "\n",
    "    # 3. Active contact bit\n",
    "    df[\"active_bit\"] = df[\"Active Contact\"].str.lower().eq(\"active\").astype(int)\n",
    "\n",
    "    # 4. Connection tier\n",
    "    df[\"connect_tier\"] = (\n",
    "        df[\"ConnectLink Status\"].str.upper()\n",
    "                              .map(CONNECTLINK_STATUS_TO_TIER)\n",
    "                              .fillna(\"1\")\n",
    "    )\n",
    "\n",
    "    # 5. Opportunity bucket\n",
    "    opps = df[\"# of opps\"].fillna(\"0\").astype(int)\n",
    "    df[\"opps_bucket\"] = pd.cut(opps, [-1, 0, 3, float(\"inf\")], labels=[\"Z\", \"L\", \"H\"]).astype(str)\n",
    "\n",
    "    # 6. Activity tier: recent (<=548 days) vs stale/blank\n",
    "    days_since_act = (reference_date - df[\"Last Activity\"]).dt.days\n",
    "    df[\"activity_tier\"] = pd.Series([\"1\"] * len(df), index=df.index)\n",
    "    recent_mask = days_since_act <= 548\n",
    "    df.loc[recent_mask, \"activity_tier\"] = \"2\"\n",
    "\n",
    "    # 7. Primary email presence bit\n",
    "    df[\"primary_email_bit\"] = df[\"Email\"].astype(str).str.strip().ne(\"\").astype(int)\n",
    "\n",
    "    # 8. Connect email presence bit\n",
    "    df[\"connect_email_bit\"] = df[\"Connect Link Email\"].astype(str).str.strip().ne(\"\").astype(int)\n",
    "\n",
    "    # 9. Creation seniority rank (days since creation, zero-padded)\n",
    "    days_since_created = (\n",
    "        reference_date - df[\"Created Date\"]\n",
    "    ).dt.days.fillna(0).clip(0, 99999).astype(int)\n",
    "    df[\"created_rank\"] = days_since_created.astype(str).str.zfill(5)\n",
    "\n",
    "    # Combine into hierarchy tag\n",
    "    df[\"hier_tag\"] = (\n",
    "        df[\"is_privileged\"].astype(int).astype(str) + \"|\" +\n",
    "        df[\"primary_bit\"].astype(str)       + \"|\" +\n",
    "        df[\"active_bit\"].astype(str)        + \"|\" +\n",
    "        df[\"connect_tier\"]                  + \"|\" +\n",
    "        df[\"opps_bucket\"]                   + \"|\" +\n",
    "        df[\"activity_tier\"]                 + \"|\" +\n",
    "        df[\"primary_email_bit\"].astype(str)+ \"|\" +\n",
    "        df[\"connect_email_bit\"].astype(str)+ \"|\" +\n",
    "        df[\"created_rank\"]\n",
    "    )\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df = df.drop(columns=[\n",
    "        \"primary_bit\", \"active_bit\", \"connect_tier\",\n",
    "        \"opps_bucket\", \"activity_tier\",\n",
    "        \"primary_email_bit\", \"connect_email_bit\", \"created_rank\"\n",
    "    ])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0f29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 3: Normalized-Fields Preparation\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def prepare_normalized_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds helper columns for blocking and fuzzy matching:\n",
    "      • email_norm: lowercased, stripped primary email\n",
    "      • connect_norm: lowercased, stripped Connect Link email\n",
    "      • name_norm: lowercased, letters-and-spaces-only full name\n",
    "      • sfi_key: blocking key of surname + '_' + first initial\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"email_norm\"]   = df[\"Email\"].astype(str).str.lower().str.strip()\n",
    "    df[\"connect_norm\"] = df[\"Connect Link Email\"].astype(str).str.lower().str.strip()\n",
    "    df[\"name_norm\"]    = (\n",
    "        df[\"Full Name\"].astype(str)\n",
    "                      .str.lower()\n",
    "                      .str.replace(r\"[^a-z ]\", \"\", regex=True)\n",
    "                      .str.strip()\n",
    "    )\n",
    "    def make_sfi(name: str) -> str:\n",
    "        parts = name.split()\n",
    "        return \"\" if len(parts) < 2 else f\"{parts[-1]}_{parts[0][0]}\"\n",
    "    df[\"sfi_key\"]      = df[\"name_norm\"].apply(make_sfi)\n",
    "    logger.info(\"Prepared normalized fields for clustering\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "598ee29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 4: Duplicate‑Candidate Clustering\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Union‑Find (Disjoint Set) data structure to group record indices\n",
    "    when they satisfy blocking or fuzzy‑match rules.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.parent: Dict[int,int] = {}\n",
    "    def find(self, x: int) -> int:\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    def union(self, x: int, y: int) -> None:\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        if root_x != root_y:\n",
    "            self.parent[root_y] = root_x\n",
    "\n",
    "\n",
    "def cluster_records(\n",
    "    df: pd.DataFrame,\n",
    "    name_threshold: int = NAME_SIMILARITY_THRESHOLD,\n",
    "    email_dist: int   = EMAIL_EDIT_DISTANCE_THRESHOLD\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assigns each record a 'dupe_cluster_id' based on these rules within each\n",
    "    Account Name:\n",
    "      1) Exact match on primary email_norm (only non‑blank emails)\n",
    "      2) One‑character‑off primary email local‑part (same domain)\n",
    "      3) Exact match on name_norm\n",
    "      4) Fuzzy match on name_norm (token_sort_ratio ≥ name_threshold)\n",
    "    Ignores connect_norm clustering. Blank emails are skipped in step 1.\n",
    "    Returns a DataFrame with new column 'dupe_cluster_id'.\n",
    "    \"\"\"\n",
    "    df = prepare_normalized_fields(df)\n",
    "    uf = UnionFind()\n",
    "\n",
    "    # Cluster within each account group\n",
    "    for acct, grp in df.groupby(\"Account Name\"):\n",
    "        indices = list(grp.index)\n",
    "\n",
    "        # 1) Exact primary email (skip blanks)\n",
    "        non_blank = grp[grp[\"email_norm\"].ne(\"\")]\n",
    "        for _, block in non_blank.groupby(\"email_norm\"):\n",
    "            ids = list(block.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 2) One‑character‑off local‑part of primary email\n",
    "        valid = non_blank[non_blank[\"email_norm\"].str.contains(\"@\", na=False)]\n",
    "        parts = valid.assign(\n",
    "            domain = valid[\"email_norm\"].str.split(\"@\").str[1],\n",
    "            local  = valid[\"email_norm\"].str.split(\"@\").str[0]\n",
    "        )\n",
    "        for _, dgrp in parts.groupby(\"domain\"):\n",
    "            idxs = list(dgrp.index)\n",
    "            for i in range(len(idxs)):\n",
    "                for j in range(i+1, len(idxs)):\n",
    "                    if distance.Levenshtein.distance(\n",
    "                        dgrp.at[idxs[i], \"local\"],\n",
    "                        dgrp.at[idxs[j], \"local\"]\n",
    "                    ) <= email_dist:\n",
    "                        uf.union(idxs[i], idxs[j])\n",
    "\n",
    "        # 3) Exact full‑name match\n",
    "        for _, block in grp.groupby(\"name_norm\"):\n",
    "            ids = list(block.index)\n",
    "            for i in ids[1:]:\n",
    "                uf.union(ids[0], i)\n",
    "\n",
    "        # 4) Fuzzy full‑name matching\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(i+1, len(indices)):\n",
    "                a, b = indices[i], indices[j]\n",
    "                if uf.find(a) != uf.find(b):\n",
    "                    if fuzz.token_sort_ratio(\n",
    "                        df.at[a, \"name_norm\"],\n",
    "                        df.at[b, \"name_norm\"]\n",
    "                    ) >= name_threshold:\n",
    "                        uf.union(a, b)\n",
    "\n",
    "        # 5) Exact sfi_key match (surname + '_' + first initial)\n",
    "        for _, block in grp.groupby(\"sfi_key\"):\n",
    "            ids = block.index.tolist()\n",
    "        for i in ids[1:]:\n",
    "            uf.union(ids[0], i)\n",
    "\n",
    "\n",
    "    # Assign stable cluster IDs\n",
    "    root_to_cid: Dict[int,str] = {}\n",
    "    cluster_ids: List[str] = []\n",
    "    counter = 1\n",
    "    for idx in df.index:\n",
    "        root = uf.find(idx)\n",
    "        if root not in root_to_cid:\n",
    "            root_to_cid[root] = f\"C{counter:05d}\"\n",
    "            counter += 1\n",
    "        cluster_ids.append(root_to_cid[root])\n",
    "    df[\"dupe_cluster_id\"] = cluster_ids\n",
    "\n",
    "    logger.info(\"Assigned dupe_cluster_id to %d records\", len(df))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa8b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 5: Canonical Selection\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def select_canonical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Within each dupe_cluster_id group, identify the record(s) with the highest\n",
    "    'hier_tag'. If there is exactly one such record, mark it as 'keep'. If there\n",
    "    are multiple tied at the top tag, mark them 'keep_tie'. All other records\n",
    "    get marked 'merge' and pointed at the chosen canonical Contact Id.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of input with new columns:\n",
    "          - is_canonical (bool)\n",
    "          - canonical_contact_id (str)\n",
    "          - resolution_status (str)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"is_canonical\"] = False\n",
    "    df[\"canonical_contact_id\"] = None\n",
    "    df[\"resolution_status\"] = None\n",
    "\n",
    "    def pick_top(sub: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        sorted_sub = sub.sort_values(\"hier_tag\", ascending=False)\n",
    "        top_tag = sorted_sub.iloc[0][\"hier_tag\"]\n",
    "        tied = sorted_sub[sorted_sub[\"hier_tag\"] == top_tag]\n",
    "        return tied, sorted_sub\n",
    "\n",
    "    for cluster_id, group in df.groupby(\"dupe_cluster_id\"):\n",
    "        tied, sorted_group = pick_top(group)\n",
    "        if len(tied) > 1:\n",
    "            # Multiple top-tier records: all are kept, others merged\n",
    "            for idx in tied.index:\n",
    "                df.at[idx, \"is_canonical\"] = True\n",
    "                df.at[idx, \"canonical_contact_id\"] = df.at[idx, \"Contact Id\"]\n",
    "                df.at[idx, \"resolution_status\"] = \"keep_tie\"\n",
    "            loser_idx = sorted_group.index.difference(tied.index)\n",
    "            for idx in loser_idx:\n",
    "                df.at[idx, \"canonical_contact_id\"] = tied.iloc[0][\"Contact Id\"]\n",
    "                df.at[idx, \"resolution_status\"] = \"merge\"\n",
    "        else:\n",
    "            # Single winner: keep it, merge the rest\n",
    "            winner_idx = tied.index[0]\n",
    "            df.at[winner_idx, \"is_canonical\"] = True\n",
    "            df.at[winner_idx, \"canonical_contact_id\"] = df.at[winner_idx, \"Contact Id\"]\n",
    "            df.at[winner_idx, \"resolution_status\"] = \"keep\"\n",
    "            loser_idx = sorted_group.index.difference([winner_idx])\n",
    "            for idx in loser_idx:\n",
    "                df.at[idx, \"canonical_contact_id\"] = df.at[winner_idx, \"Contact Id\"]\n",
    "                df.at[idx, \"resolution_status\"] = \"merge\"\n",
    "    logger.info(\"Selected canonical records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7522ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 6: Merge or Inactivate\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def merge_or_inactivate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each record not marked canonical, applies merge/inactivate logic uniformly,\n",
    "    with:\n",
    "      • special handling when the keep has a blank primary email (unchanged),\n",
    "      • and in the standard case (keep has a primary), merges only on primary→primary\n",
    "        (exact or Levenshtein ≤ threshold) and immediately inactivates any other non-blank.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Privileged safeguard\n",
    "    priv = df[\"is_privileged\"]\n",
    "    df.loc[priv, \"is_canonical\"]          = True\n",
    "    df.loc[priv, \"canonical_contact_id\"]  = df.loc[priv, \"Contact Id\"]\n",
    "    df.loc[priv, \"resolution_status\"]     = \"keep_privileged\"\n",
    "\n",
    "    # 2) Gather canonical info per cluster\n",
    "    canonals: Dict[str, List[Dict[str,str]]] = {}\n",
    "    for cid, grp in df[df[\"is_canonical\"]].groupby(\"dupe_cluster_id\"):\n",
    "        canonals[cid] = [{\n",
    "            \"primary\":      df.at[idx, \"email_norm\"],\n",
    "            \"connect\":      df.at[idx, \"connect_norm\"],\n",
    "            \"name\":         df.at[idx, \"name_norm\"],\n",
    "            \"contact_id\":   df.at[idx, \"Contact Id\"],\n",
    "            \"hier_tag\":     df.at[idx, \"hier_tag\"]\n",
    "        } for idx in grp.index]\n",
    "\n",
    "    # 3) Identify candidates\n",
    "    candidates: Dict[str,List[int]] = {}\n",
    "    for cid, grp in df.groupby(\"dupe_cluster_id\"):\n",
    "        lst = [i for i in grp.index\n",
    "               if not df.at[i,\"is_canonical\"] and not df.at[i,\"is_privileged\"]]\n",
    "        if lst:\n",
    "            candidates[cid] = lst\n",
    "\n",
    "    # 4) Special‑case: keep with blank primary email (unchanged)\n",
    "    for cid, cans in candidates.items():\n",
    "        keeps = canonals.get(cid, [])\n",
    "        if len(keeps)==1 and keeps[0][\"primary\"] == \"\":\n",
    "            keep_id = keeps[0][\"contact_id\"]\n",
    "            # single candidate → merge\n",
    "            if len(cans)==1:\n",
    "                df.at[cans[0], \"resolution_status\"]        = \"merge_exact\"\n",
    "                df.at[cans[0], \"canonical_contact_id\"]    = keep_id\n",
    "                continue\n",
    "            # multiple candidates → choose by connect match or hier_tag\n",
    "            keep_con = keeps[0][\"connect\"]\n",
    "            # 4a) primary==keep.connect\n",
    "            prim = [i for i in cans if df.at[i,\"email_norm\"] == keep_con]\n",
    "            if len(prim)==1:\n",
    "                winner = prim[0]\n",
    "            elif prim:\n",
    "                winner = max(prim, key=lambda i: df.at[i,\"hier_tag\"])\n",
    "            else:\n",
    "                # 4b) connect==keep.connect\n",
    "                conn = [i for i in cans if df.at[i,\"connect_norm\"] == keep_con]\n",
    "                if len(conn)==1:\n",
    "                    winner = conn[0]\n",
    "                elif conn:\n",
    "                    winner = max(conn, key=lambda i: df.at[i,\"hier_tag\"])\n",
    "                else:\n",
    "                    winner = max(cans, key=lambda i: df.at[i,\"hier_tag\"])\n",
    "            # apply merges/inactivations\n",
    "            for i in cans:\n",
    "                if i == winner:\n",
    "                    df.at[i, \"resolution_status\"]        = \"merge_exact\"\n",
    "                    df.at[i, \"canonical_contact_id\"]    = keep_id\n",
    "                else:\n",
    "                    df.at[i, \"resolution_status\"]        = \"inactive\"\n",
    "                    df.at[i, \"canonical_contact_id\"]    = None\n",
    "            continue\n",
    "\n",
    "    # 5) Standard logic: keep has a primary email\n",
    "    for idx, row in df[(~df[\"is_canonical\"]) & (~df[\"is_privileged\"])].iterrows():\n",
    "        # skip if already set\n",
    "        if pd.notna(row[\"resolution_status\"]):\n",
    "            continue\n",
    "\n",
    "        cid      = row[\"dupe_cluster_id\"]\n",
    "        S        = canonals.get(cid, [])\n",
    "        if not S:\n",
    "            continue\n",
    "\n",
    "        keep_pri = S[0][\"primary\"]\n",
    "        r_pri    = row[\"email_norm\"]\n",
    "\n",
    "        # 5a) If keep_pri is non-blank, only merge on primary→primary (exact or one-char-off)\n",
    "        if keep_pri:\n",
    "            if r_pri:\n",
    "                dist = distance.Levenshtein.distance\n",
    "                if r_pri == keep_pri or dist(r_pri, keep_pri) <= EMAIL_EDIT_DISTANCE_THRESHOLD:\n",
    "                    df.at[idx, \"resolution_status\"]       = \"merge_exact\"\n",
    "                    df.at[idx, \"canonical_contact_id\"]   = S[0][\"contact_id\"]\n",
    "                else:\n",
    "                    df.at[idx, \"resolution_status\"]       = \"inactive\"\n",
    "                continue\n",
    "            # else r_pri is blank → fall through to name logic\n",
    "\n",
    "        # 5b) Name-based for truly blank primary\n",
    "        matched = False\n",
    "        for C in S:\n",
    "            if row[\"name_norm\"] == C[\"name\"]:\n",
    "                df.at[idx, \"resolution_status\"]       = \"merge_name_exact\"\n",
    "                df.at[idx, \"canonical_contact_id\"]   = C[\"contact_id\"]\n",
    "                matched = True\n",
    "                break\n",
    "        if matched:\n",
    "            continue\n",
    "\n",
    "        for C in S:\n",
    "            if fuzz.token_sort_ratio(row[\"name_norm\"], C[\"name\"]) >= NAME_SIMILARITY_THRESHOLD:\n",
    "                df.at[idx, \"resolution_status\"]       = \"merge_name_fuzzy\"\n",
    "                df.at[idx, \"canonical_contact_id\"]   = C[\"contact_id\"]\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            df.at[idx, \"resolution_status\"] = \"WIP\"\n",
    "\n",
    "    logger.info(\"Applied updated merge/inactivate logic\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfd2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_primary_merge_threshold(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Post‑check: for any row that was merged (merge_exact, merge_name_*, etc.),\n",
    "    if its primary email_norm and its keep’s email_norm differ by >1 char,\n",
    "    mark it inactive instead.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # build lookup of Contact Id → keep’s primary email_norm\n",
    "    keep_email = df[df[\"is_canonical\"]].set_index(\"Contact Id\")[\"email_norm\"].to_dict()\n",
    "    thresh = EMAIL_EDIT_DISTANCE_THRESHOLD\n",
    "    dist   = distance.Levenshtein.distance\n",
    "\n",
    "    for idx, row in df[df[\"resolution_status\"].str.startswith(\"merge\")].iterrows():\n",
    "        r_pri = row[\"email_norm\"]\n",
    "        keep_id = row[\"canonical_contact_id\"]\n",
    "        c_pri = keep_email.get(keep_id, \"\")\n",
    "        # only enforce on non‑blank primaries\n",
    "        if r_pri and c_pri and dist(r_pri, c_pri) > thresh:\n",
    "            df.at[idx, \"resolution_status\"] = \"inactive\"\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fee5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 6c: Re‑assign exact‑email inactives back to merge_exact\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def reassign_inactive_merges(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Any row marked 'inactive' but whose primary email_norm exactly matches\n",
    "    a keep's primary email_norm in the same dupe_cluster_id will be flipped\n",
    "    to 'merge_exact' against that keep.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Build lookup: (dupe_cluster_id, primary_email) -> keep Contact Id\n",
    "    keep_lookup = (\n",
    "        df[df[\"is_canonical\"] & df[\"email_norm\"].ne(\"\")]\n",
    "          .groupby([\"dupe_cluster_id\", \"email_norm\"])[\"Contact Id\"]\n",
    "          .first()\n",
    "          .to_dict()\n",
    "    )\n",
    "\n",
    "    # Find all inactive rows with a non‑blank primary email\n",
    "    mask = (df[\"resolution_status\"] == \"inactive\") & df[\"email_norm\"].ne(\"\")\n",
    "    for idx in df[mask].index:\n",
    "        key = (df.at[idx, \"dupe_cluster_id\"], df.at[idx, \"email_norm\"])\n",
    "        if key in keep_lookup:\n",
    "            df.at[idx, \"resolution_status\"]      = \"merge_exact\"\n",
    "            df.at[idx, \"canonical_contact_id\"]   = keep_lookup[key]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f6ad201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 7: Feature Engineering for Machine Learning\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def extract_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Constructs a feature matrix and label vector from the processed DataFrame:\n",
    "      - Numeric versions of hierarchy bits (privilege, primary, active)\n",
    "      - Numeric connect tier, opportunity bucket, activity days, creation days\n",
    "      - Name/email similarity to chosen canonical record\n",
    "      - Cluster size\n",
    "    Labels are taken from 'resolution_status' for supervised learning.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.Series): Encoded labels\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Ensure name_norm and email_norm exist\n",
    "    df = prepare_normalized_fields(df)\n",
    "    # Identify canonical per cluster\n",
    "    canon_map: Dict[str, Dict[str, str]] = {}\n",
    "    for cid, group in df[df[\"is_canonical\"]].groupby(\"dupe_cluster_id\"):\n",
    "        canon_map[cid] = {\n",
    "            \"email_norm\": group.iloc[0][\"email_norm\"],\n",
    "            \"name_norm\":  group.iloc[0][\"name_norm\"]\n",
    "        }\n",
    "    # Compute features\n",
    "    feature_dicts: List[Dict[str, float]] = []\n",
    "    labels: List[str] = []\n",
    "    for idx, row in df.iterrows():\n",
    "        cid = row[\"dupe_cluster_id\"]\n",
    "        canon_vals = canon_map.get(cid, {\"email_norm\":\"\",\"name_norm\":\"\"})\n",
    "        # Hierarchy bits\n",
    "        feat: Dict[str, float] = {}\n",
    "        feat[\"is_privileged\"] = float(row[\"is_privileged\"])\n",
    "        feat[\"primary_bit\"]    = float(row[\"Primary Contact\"])\n",
    "        feat[\"active_bit\"]     = float(row[\"Active Contact\"].lower()==\"active\")\n",
    "        # Connect tier numeric\n",
    "        feat[\"connect_tier\"]   = float(CONNECTLINK_STATUS_TO_TIER.get(row[\"ConnectLink Status\"].upper(), \"1\"))\n",
    "        # Opportunity bucket numeric\n",
    "        opps = int(row[\"# of opps\"] or 0)\n",
    "        feat[\"opps_bucket\"]    = float(0 if opps==0 else 1 if opps<=3 else 2)\n",
    "        # Activity recency days\n",
    "        days_act = (pd.Timestamp.today().normalize() - row[\"Last Activity\"]).days if pd.notna(row[\"Last Activity\"]) else 0\n",
    "        feat[\"days_since_activity\"] = float(days_act)\n",
    "        # Creation seniority days\n",
    "        days_cr = (pd.Timestamp.today().normalize() - row[\"Created Date\"]).days if pd.notna(row[\"Created Date\"]) else 0\n",
    "        feat[\"days_since_created\"] = float(days_cr)\n",
    "        # Similarities to canonical\n",
    "        feat[\"name_similarity\"] = float(fuzz.token_sort_ratio(row[\"name_norm\"], canon_vals[\"name_norm\"]))\n",
    "        feat[\"email_edit_dist\"] = float(distance.Levenshtein.distance(row[\"email_norm\"], canon_vals[\"email_norm\"]))\n",
    "        # Cluster size\n",
    "        feat[\"cluster_size\"]    = float(len(df[df[\"dupe_cluster_id\"]==cid]))\n",
    "        feature_dicts.append(feat)\n",
    "        labels.append(row[\"resolution_status\"])\n",
    "    X = pd.DataFrame(feature_dicts)\n",
    "    # Encode string labels to integers\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels)\n",
    "    # Persist label encoder for inference\n",
    "    df_label_enc = pd.DataFrame({\"label\": labels})\n",
    "    return X, pd.Series(y), encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f59cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 8: Model Training, Evaluation, Saving\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def train_and_save_model(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    encoder: LabelEncoder,\n",
    "    model_path: Path,\n",
    "    encoder_path: Path\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Splits features and labels into train/test, trains a RandomForest classifier,\n",
    "    evaluates performance, and saves both the trained model and label encoder.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    logger.info(\"Model evaluation:\\n%s\", report)\n",
    "\n",
    "    # Save model and encoder\n",
    "    joblib.dump(clf, model_path)\n",
    "    joblib.dump(encoder, encoder_path)\n",
    "    logger.info(\"Saved trained model to %s and encoder to %s\", model_path, encoder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "643b8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 9: Model Loading and Inference\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def load_and_apply_model(\n",
    "    df: pd.DataFrame,\n",
    "    model_path: Path,\n",
    "    encoder_path: Path\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a saved classifier and label encoder, computes features for the DataFrame,\n",
    "    predicts resolution_status for each record, and appends a new column\n",
    "    'ml_resolution_status' with the decoded predictions.\n",
    "    \"\"\"\n",
    "    # Load model and encoder\n",
    "    clf: RandomForestClassifier = joblib.load(model_path)\n",
    "    encoder: LabelEncoder = joblib.load(encoder_path)\n",
    "\n",
    "    # Extract features (we discard the returned encoder)\n",
    "    X, _, _ = extract_features(df)\n",
    "    y_pred = clf.predict(X)\n",
    "    decoded = encoder.inverse_transform(y_pred)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"ml_resolution_status\"] = decoded\n",
    "    logger.info(\"Applied ML model to %d records\", len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ccdd284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Full Pipeline Invocation\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def run_pipeline(\n",
    "    input_path: Path,\n",
    "    output_path: Path,\n",
    "    model_path: Optional[Path]   = None,\n",
    "    encoder_path: Optional[Path] = None,\n",
    "    train_model_flag: bool       = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executes the full pipeline:\n",
    "      1) Ingest contacts\n",
    "      2) Build hierarchy tags\n",
    "      3) Cluster duplicates\n",
    "      4) Select canonical records\n",
    "      5) Merge or inactivate others\n",
    "      6) Optionally train and save an ML model on this output\n",
    "      7) Optionally load and apply an existing ML model\n",
    "\n",
    "    Parameters:\n",
    "        input_path (Path): Path to raw Excel input.\n",
    "        output_path (Path): Path where results (and Excel export) will be written.\n",
    "        model_path (Path, optional): Where to save or load the trained classifier.\n",
    "        encoder_path (Path, optional): Where to save or load the label encoder.\n",
    "        train_model_flag (bool): If True, trains a model on current data and saves it.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Final DataFrame with both rule-based and ML-based resolution statuses.\n",
    "    \"\"\"\n",
    "    df = load_contacts(input_path)\n",
    "    df = build_hierarchy_tag(df)\n",
    "    df = cluster_records(df)\n",
    "    df = select_canonical(df)\n",
    "    df = merge_or_inactivate(df)\n",
    "    df = enforce_primary_merge_threshold(df)\n",
    "    df = reassign_inactive_merges(df)\n",
    "\n",
    "    # Optional training\n",
    "    if train_model_flag and model_path and encoder_path:\n",
    "        X, y, encoder = extract_features(df)\n",
    "        train_and_save_model(X, y, encoder, model_path, encoder_path)\n",
    "    # Optional inference\n",
    "    if model_path and encoder_path and not train_model_flag:\n",
    "        if model_path.exists() and encoder_path.exists():\n",
    "            df = load_and_apply_model(df, model_path, encoder_path)\n",
    "        else:\n",
    "            logger.warning(\n",
    "            \"Model or encoder file not found at %s or %s; skipping ML inference\",\n",
    "            model_path, encoder_path\n",
    "        )\n",
    "\n",
    "  # Export logic: write full results to Excel for comparison\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "        # Full dataset including rule-based and ML-based statuses\n",
    "        df.to_excel(writer, sheet_name=\"results\", index=False)\n",
    "    logger.info(\"Exported results to %s\", output_path)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9904113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded and normalized 46523 rows\n",
      "INFO:__main__:Prepared normalized fields for clustering\n",
      "INFO:__main__:Assigned dupe_cluster_id to 46523 records\n",
      "INFO:__main__:Selected canonical records\n",
      "INFO:__main__:Applied updated merge/inactivate logic\n",
      "WARNING:__main__:Model or encoder file not found at models\\rf_model.joblib or models\\label_encoder.joblib; skipping ML inference\n",
      "INFO:__main__:Exported results to output\\dedup_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "df = run_pipeline(\n",
    "    input_path=Path(\"../data/Duplicate Contact Scrub.xlsx\"),\n",
    "    output_path=Path(\"output/dedup_results.xlsx\"),\n",
    "    model_path=Path(\"models/rf_model.joblib\"),\n",
    "    encoder_path=Path(\"models/label_encoder.joblib\"),\n",
    "    train_model_flag=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
